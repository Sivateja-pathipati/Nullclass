{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":365728,"sourceType":"datasetVersion","datasetId":159647},{"sourceId":915334,"sourceType":"datasetVersion","datasetId":492138},{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090},{"sourceId":4246862,"sourceType":"datasetVersion","datasetId":2502545},{"sourceId":6190858,"sourceType":"datasetVersion","datasetId":3553479},{"sourceId":7530378,"sourceType":"datasetVersion","datasetId":4385984},{"sourceId":9446283,"sourceType":"datasetVersion","datasetId":5741075},{"sourceId":9460732,"sourceType":"datasetVersion","datasetId":5751733}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport random\nimport re\nimport json\n# import contractions\nfrom collections import Counter\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T14:22:14.980167Z","iopub.execute_input":"2024-09-26T14:22:14.980615Z","iopub.status.idle":"2024-09-26T14:22:28.743951Z","shell.execute_reply.started":"2024-09-26T14:22:14.980533Z","shell.execute_reply":"2024-09-26T14:22:28.742837Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/english-vocab/words_alpha.txt\n/kaggle/input/hindinlp/HindiNLP.csv\n/kaggle/input/samanantar-en-to-hi-1/dataset_1.csv\n/kaggle/input/english-hindi-dataset/Dataset_English_Hindi.csv\n/kaggle/input/english-to-hindi-parallel-dataset/newdata.csv\n/kaggle/input/english-to-hindi-dataset/english_to_hindi.txt\n/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"np.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:28.746031Z","iopub.execute_input":"2024-09-26T14:22:28.747104Z","iopub.status.idle":"2024-09-26T14:22:28.752298Z","shell.execute_reply.started":"2024-09-26T14:22:28.747048Z","shell.execute_reply":"2024-09-26T14:22:28.751285Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#english_dictionary\npath0 = \"/kaggle/input/english-vocab/words_alpha.txt\"\n#main_vocab_files\npath1 = '/kaggle/input/hindinlp/HindiNLP.csv'\npath2 = \"/kaggle/input/english-to-hindi-dataset/english_to_hindi.txt\"\n#samanantar_file\npath3 = '/kaggle/input/samanantar-en-to-hi-1/dataset_1.csv'\n#newdata and dataset_english_hindi_file\npath4 = \"/kaggle/input/english-hindi-dataset/Dataset_English_Hindi.csv\"\npath5 = \"/kaggle/input/english-to-hindi-parallel-dataset/newdata.csv\"\n# IIT bombay file\npath6 = \"/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:30.849120Z","iopub.execute_input":"2024-09-26T14:22:30.850021Z","iopub.status.idle":"2024-09-26T14:22:30.855279Z","shell.execute_reply.started":"2024-09-26T14:22:30.849976Z","shell.execute_reply":"2024-09-26T14:22:30.854118Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Lowercase, trim, and remove non-letter characters\ndef normalizeEng(s):\n    s = str(s).lower().strip()\n    s = re.sub(r\"([.!?|,])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z!?.,]+\", r\" \", s)\n    s = ' '.join(s.split())\n    return s\n\ndef normalizeHin(s):\n    s = str(s).lower().strip()\n    s = re.sub(r\"([.!?|,])\", r\" \\1\", s)\n    s = re.sub(r\"([\\u0964])\", r\" \\1\", s)\n    s = re.sub(r'[^\\u0900-\\u0965\\u0970-\\u097F!?.,]+',r\" \",s)\n    s = ' '.join(s.split())\n    return s\n\ndef tf_lower_and_remove_punct(text):\n    text = tf.strings.lower(text)\n    text = tf.strings.regex_replace(text, \"[^ a-z.?,!]\", \"\")\n    text = tf.strings.regex_replace(text, \"[.?,!]\", r\" \\0 \")\n    text = tf.strings.strip(text)\n    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n    return text\n\n\ndef tf_lower_and_remove_punct_1(text):\n    text = tf.strings.regex_replace(text,'[^ \\u0900-\\u0965\\u0970-\\u097F?,!]','')\n    text = tf.strings.regex_replace(text, \"[\\u0964?,!]\", r\" \\0 \")\n    text = tf.strings.strip(text)\n    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:32.852903Z","iopub.execute_input":"2024-09-26T14:22:32.853661Z","iopub.status.idle":"2024-09-26T14:22:32.865198Z","shell.execute_reply.started":"2024-09-26T14:22:32.853617Z","shell.execute_reply":"2024-09-26T14:22:32.864233Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#function to create vocab using dataframe and a column\ndef vocab_forming(df,column):\n    vocab = df[column].to_list()\n    vocab = [i for j in vocab for i in j.split()]\n    vocab = list(Counter(vocab).items())\n    vocab = sorted(vocab,key = lambda x:x[1],reverse = True)\n    vocab = [k for k,v in vocab]\n    return vocab\n\n#function to filter rows with proper eng & hin sequence length\ndef filter_proper_seq_len(df):\n    #getting seq_lengths\n    df['eng_seq_len'] = df['english'].apply(lambda x: len(x.split()))\n    df['hin_seq_len'] = df['hindi'].apply(lambda x: len(x.split()))\n    df['diff'] = df['eng_seq_len']-df['hin_seq_len']\n    #first filter to reduce the operations\n    df = df[(df['eng_seq_len']<30)&(df['hin_seq_len']<40)]\n    \n    #dividing the seq lengths in three categories\n    #this is to allocate allowable difference in seq_lengths\n    df_a = df[(df['eng_seq_len']<=10)]\n    df_a = df_a[(df_a['diff']<3)&(df_a['diff']>-3)]\n    \n    df_b = df[(df['eng_seq_len']>10)&(df['eng_seq_len']<=20)]\n    df_b = df_b[(df_b['diff']<5)&(df_b['diff']>-5)]\n    \n    df_c = df[(df['eng_seq_len']>20)&(df['eng_seq_len']<=30)]\n    df_c = df_c[(df_c['diff']<8)&(df_c['diff']>-8)]\n    \n    df = pd.concat([df_a,df_b,df_c])\n    df = df.reset_index(drop = True)\n    return df\n\n#to extract rows containing required vocab with \ndef filtering_rows_using_vocab(text,vocab =None,temp =None):\n    text = text.split()\n    text_set = set(text)\n    vocab_in_text = len(text_set)\n    common_vocab = len(text_set&vocab)\n    if common_vocab>=temp*vocab_in_text:\n        return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:33.553038Z","iopub.execute_input":"2024-09-26T14:22:33.553452Z","iopub.status.idle":"2024-09-26T14:22:33.567465Z","shell.execute_reply.started":"2024-09-26T14:22:33.553413Z","shell.execute_reply":"2024-09-26T14:22:33.566392Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"with open(path0,'r') as f:\n    data = f.readlines()\n    data = [i.strip('\\n') for i in data]\nmain_english_vocab = set(data)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:34.080204Z","iopub.execute_input":"2024-09-26T14:22:34.081166Z","iopub.status.idle":"2024-09-26T14:22:34.320443Z","shell.execute_reply.started":"2024-09-26T14:22:34.081119Z","shell.execute_reply":"2024-09-26T14:22:34.319479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#df1 some clean sentences used in daily conversations\n#this df is used as base df1 for vocabulary and also used in val_pairs to make perfect training in the model\ndf1 =pd.read_csv(path1)\ncol = df1.columns\ndf1 = df1.rename(columns ={\n    col[0]:'english',\n    col[1]:'hindi'\n})\ndf1 = df1.dropna()\n\nprint(\"length of df of basic_texts: \",len(df1))\n\ndf1['english'] = df1['english'].apply(normalizeEng)\ndf1['hindi'] = df1['hindi'].apply(normalizeHin)\n\ndf1 = df1.drop_duplicates()\ndf1 = df1.reset_index(drop = True)\n\ndf1 = filter_proper_seq_len(df1)\nprint('length of df after filtering: ',len(df1))\neng_vocab_set_from_df1 = set(vocab_forming(df1,'english'))\nprint(len(eng_vocab_set_from_df1))\n\nhin_vocab_set_from_df1 = set(vocab_forming(df1,'hindi'))\nprint(len(hin_vocab_set_from_df1))\ndf1.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:35.330254Z","iopub.execute_input":"2024-09-26T14:22:35.330702Z","iopub.status.idle":"2024-09-26T14:22:35.460605Z","shell.execute_reply.started":"2024-09-26T14:22:35.330662Z","shell.execute_reply":"2024-09-26T14:22:35.459578Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"length of df of basic_texts:  617\nlength of df after filtering:  350\n1013\n1155\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 english                   hindi  eng_seq_len  hin_seq_len  \\\n0  hello , how are you ?  नमस्ते , आप कैसे हैं ?            6            6   \n1    what is your name ?      आपका क्या नाम है ?            5            5   \n2   where are you from ?        आप कहाँ से हैं ?            5            5   \n\n   diff  \n0     0  \n1     0  \n2     0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>hindi</th>\n      <th>eng_seq_len</th>\n      <th>hin_seq_len</th>\n      <th>diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hello , how are you ?</td>\n      <td>नमस्ते , आप कैसे हैं ?</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what is your name ?</td>\n      <td>आपका क्या नाम है ?</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>where are you from ?</td>\n      <td>आप कहाँ से हैं ?</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df1.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:36.521075Z","iopub.execute_input":"2024-09-26T14:22:36.521465Z","iopub.status.idle":"2024-09-26T14:22:36.545778Z","shell.execute_reply.started":"2024-09-26T14:22:36.521427Z","shell.execute_reply":"2024-09-26T14:22:36.544676Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       eng_seq_len  hin_seq_len        diff\ncount   350.000000   350.000000  350.000000\nmean     13.014286    13.642857   -0.628571\nstd       5.200542     5.496641    1.810281\nmin       2.000000     2.000000   -6.000000\n25%       9.000000     9.000000   -2.000000\n50%      14.000000    14.000000    0.000000\n75%      16.000000    17.000000    0.000000\nmax      29.000000    29.000000    6.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng_seq_len</th>\n      <th>hin_seq_len</th>\n      <th>diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>350.000000</td>\n      <td>350.000000</td>\n      <td>350.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13.014286</td>\n      <td>13.642857</td>\n      <td>-0.628571</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>5.200542</td>\n      <td>5.496641</td>\n      <td>1.810281</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>-6.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>9.000000</td>\n      <td>9.000000</td>\n      <td>-2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>14.000000</td>\n      <td>14.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>16.000000</td>\n      <td>17.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>29.000000</td>\n      <td>29.000000</td>\n      <td>6.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#df2 is also clean sentences that are used in daily life,\n#it is also used as base df2 for vocabulary and val_pairs to fit them perfectly\nwith open(path2,'r',encoding = 'utf-8') as f:\n    data = f.readlines()\n    data = [i.strip('\\n').split('\\t') for i in data]\ndf2 = pd.DataFrame(data,columns = ['english','hindi'])\n\n#dropping null objects\ndf2 = df2.dropna()\n\n\nprint('initial_length of df2: ', len(df2))\n\n#applying normalizeEng and Hin functions\ndf2['english'] = df2['english'].apply(normalizeEng)\ndf2['hindi'] = df2['hindi'].apply(normalizeHin)\n\n#dropping duplicates\ndf2 = df2.drop_duplicates()\n\n#filtering rows using filter_proper_seq_len funciton\ndf2 = filter_proper_seq_len(df2)\ndf2 = df2.reset_index(drop = True)\n\nprint('length of df after filtering: ',len(df2))\n\n#checking vocab length\neng_vocab_set_from_df2 = set(vocab_forming(df2,'english'))\nprint(len(eng_vocab_set_from_df2))\n\nhin_vocab_set_from_df2 = set(vocab_forming(df2,'hindi'))\nprint(len(hin_vocab_set_from_df2))\ndf2.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:37.396389Z","iopub.execute_input":"2024-09-26T14:22:37.396858Z","iopub.status.idle":"2024-09-26T14:22:38.590772Z","shell.execute_reply.started":"2024-09-26T14:22:37.396818Z","shell.execute_reply":"2024-09-26T14:22:38.589626Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"initial_length of df2:  29415\nlength of df after filtering:  26512\n10200\n11245\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  english   hindi  eng_seq_len  hin_seq_len  diff\n0  help !  बचाओ !            2            2     0\n1  jump .  उछलो .            2            2     0\n2  jump .  कूदो .            2            2     0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>hindi</th>\n      <th>eng_seq_len</th>\n      <th>hin_seq_len</th>\n      <th>diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>help !</td>\n      <td>बचाओ !</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>jump .</td>\n      <td>उछलो .</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>jump .</td>\n      <td>कूदो .</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df2.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:38.592688Z","iopub.execute_input":"2024-09-26T14:22:38.593006Z","iopub.status.idle":"2024-09-26T14:22:38.615511Z","shell.execute_reply.started":"2024-09-26T14:22:38.592972Z","shell.execute_reply":"2024-09-26T14:22:38.614608Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        eng_seq_len   hin_seq_len          diff\ncount  26512.000000  26512.000000  26512.000000\nmean       3.107876      3.327399     -0.219523\nstd        2.398979      2.530312      0.829885\nmin        1.000000      1.000000     -6.000000\n25%        2.000000      2.000000     -1.000000\n50%        2.000000      2.000000      0.000000\n75%        4.000000      4.000000      0.000000\nmax       26.000000     28.000000      4.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng_seq_len</th>\n      <th>hin_seq_len</th>\n      <th>diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>26512.000000</td>\n      <td>26512.000000</td>\n      <td>26512.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.107876</td>\n      <td>3.327399</td>\n      <td>-0.219523</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.398979</td>\n      <td>2.530312</td>\n      <td>0.829885</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>-6.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>26.000000</td>\n      <td>28.000000</td>\n      <td>4.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#main english and hindi vocab to filter the rows from other dataset\nselected_english_vocab = eng_vocab_set_from_df2.union(eng_vocab_set_from_df1)\nprint('selected_english_vocab_length: ',len(selected_english_vocab))\nprint('selected_english_vocab_length_in_dict: ',len(selected_english_vocab&main_english_vocab))\nselected_hindi_vocab = hin_vocab_set_from_df2.union(hin_vocab_set_from_df1)\nprint('selected_hindi_vocab_length: ',len(selected_hindi_vocab))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:39.371120Z","iopub.execute_input":"2024-09-26T14:22:39.371584Z","iopub.status.idle":"2024-09-26T14:22:39.382380Z","shell.execute_reply.started":"2024-09-26T14:22:39.371529Z","shell.execute_reply":"2024-09-26T14:22:39.381369Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"selected_english_vocab_length:  10511\nselected_english_vocab_length_in_dict:  8032\nselected_hindi_vocab_length:  11645\n","output_type":"stream"}]},{"cell_type":"code","source":"#df3 data frame \ndf3 = pd.read_csv(path3)\nprint(len(df3))\ndf3 = df3[['src','tgt']]\ncol = df3.columns\ndf3 = df3.rename(columns = {\n    col[0]:'english',\n    col[1]:'hindi'\n})\ndf3.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:40.435899Z","iopub.execute_input":"2024-09-26T14:22:40.436653Z","iopub.status.idle":"2024-09-26T14:22:53.458992Z","shell.execute_reply.started":"2024-09-26T14:22:40.436607Z","shell.execute_reply":"2024-09-26T14:22:53.457883Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1265714\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                        english  \\\n0             Add sugar and muddle the mixture.   \n1                               We can't, Coop.   \n2  The court had delivered the order last week.   \n\n                                     hindi  \n0    इस मिश्रण को छानकर इसमें चीनी मिलाएं।  \n1                  हम नहीं देख सकते, Coop.  \n2  हाई कोर्ट ने यह आदेश पिछले सप्ताह दिया।  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>hindi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Add sugar and muddle the mixture.</td>\n      <td>इस मिश्रण को छानकर इसमें चीनी मिलाएं।</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We can't, Coop.</td>\n      <td>हम नहीं देख सकते, Coop.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The court had delivered the order last week.</td>\n      <td>हाई कोर्ट ने यह आदेश पिछले सप्ताह दिया।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#df4 data frame\ndf4 = pd.read_csv(path4)\nprint(len(df4))\ncol = df4.columns\ndf4 = df4.rename(columns = {\n    col[0]:'english',\n    col[1]:'hindi'\n})\ndf4.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:53.460750Z","iopub.execute_input":"2024-09-26T14:22:53.461072Z","iopub.status.idle":"2024-09-26T14:22:54.547504Z","shell.execute_reply.started":"2024-09-26T14:22:53.461038Z","shell.execute_reply":"2024-09-26T14:22:54.546596Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"130476\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"  english  hindi\n0   Help!  बचाओ!\n1   Jump.  उछलो.\n2   Jump.  कूदो.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>hindi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Help!</td>\n      <td>बचाओ!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Jump.</td>\n      <td>उछलो.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jump.</td>\n      <td>कूदो.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#df5 dataframe\ndf5 = pd.read_csv(path5,index_col =0)\nprint(len(df5))\ncol = df5.columns\ndf5 = df5.rename(columns = {\n    col[0]:'english',\n    col[1]:'hindi'\n})\ndf5.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:54.548831Z","iopub.execute_input":"2024-09-26T14:22:54.549164Z","iopub.status.idle":"2024-09-26T14:22:56.167688Z","shell.execute_reply.started":"2024-09-26T14:22:54.549128Z","shell.execute_reply":"2024-09-26T14:22:56.166614Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"177606\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                             english  \\\n0  politicians do not have permission to do what ...   \n1         I'd like to tell you about one such child,   \n2  This percentage is even greater than the perce...   \n\n                                               hindi  \n0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>hindi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>politicians do not have permission to do what ...</td>\n      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'd like to tell you about one such child,</td>\n      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This percentage is even greater than the perce...</td>\n      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#df6 dataframe\ndf6 = pd.read_csv(path6)\ndf6 = df6[['english','hindi']]\nprint(len(df6))\ndf6.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:22:56.170201Z","iopub.execute_input":"2024-09-26T14:22:56.171009Z","iopub.status.idle":"2024-09-26T14:23:07.504734Z","shell.execute_reply.started":"2024-09-26T14:22:56.170959Z","shell.execute_reply":"2024-09-26T14:23:07.503422Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"1561841\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                          english  \\\n0  Give your application an accessibility workout   \n1               Accerciser Accessibility Explorer   \n2  The default plugin layout for the bottom panel   \n3     The default plugin layout for the top panel   \n4  A list of plugins that are disabled by default   \n\n                                               hindi  \n0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें  \n1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक  \n2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका  \n3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका  \n4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>hindi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Give your application an accessibility workout</td>\n      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Accerciser Accessibility Explorer</td>\n      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The default plugin layout for the bottom panel</td>\n      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The default plugin layout for the top panel</td>\n      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A list of plugins that are disabled by default</td>\n      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#pipeline_0 for droping cleaning and extracting rows with proper sequence length\ndef pipeline_0(df_n):\n    print('current_running_pipeline_0:')\n    print('initial length of the dataframe: ',len(df_n))\n    df = df_n.copy()\n    df = df.dropna()\n    df = df.reset_index(drop = True)\n    \n    #removing data with englishalpha and hindi_alpha\n    english_alpha =df[df['hindi'].apply(lambda x: True if len(re.findall('[a-zA-Z]',x)) else False)]\n    df = df.drop(english_alpha.index)\n    df = df.reset_index(drop = True)\n    hindi_alpha = df[df['english'].apply(lambda x: True if len(re.findall('[\\u0900-\\u097F]',x)) else False)]\n    df = df.drop(hindi_alpha.index)\n    df = df.reset_index(drop = True)\n\n\n    \n    #initial filtering using sequence lenghts\n    df['eng_seq_len'] = df['english'].apply(lambda x: len(x.split()))\n    df['hin_seq_len'] = df['hindi'].apply(lambda x: len(x.split()))\n    df['diff'] = df['eng_seq_len']-df['hin_seq_len']\n    df = df[(df['eng_seq_len']<30)&(df['hin_seq_len']<30)]\n    df = df[(df['diff']<10)&(df['diff']>-10)]\n    \n    # extracting respective alhpabets\n    df['english'] = df['english'].apply(normalizeEng)\n    df['hindi'] = df['hindi'].apply(normalizeHin)\n\n    #dropping improper rows_1\n    empty_columns = df[(df['english']=='')|(df['hindi'] == '')]\n    df = df.drop(empty_columns.index)\n        \n    #dropping improper rows_2\n    indexes1 =df[(df['english'].str.len()>20)&(df['hindi'].str.len()<7)].index \n    df = df.drop(indexes1)\n    \n    #dropping improper rows_3\n    indexes2 = df[(df['english'].str.len()<8)&(df['hindi'].str.len()>14)].index\n    df = df.drop(indexes2)\n\n    #filtering again using seq_lengths using filter_proper_seq_len function\n    df = filter_proper_seq_len(df)\n    \n    print('length of df after proper_cleaning: ',len(df))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:23:08.805464Z","iopub.execute_input":"2024-09-26T14:23:08.805999Z","iopub.status.idle":"2024-09-26T14:23:08.824562Z","shell.execute_reply.started":"2024-09-26T14:23:08.805947Z","shell.execute_reply":"2024-09-26T14:23:08.823506Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#filtering rows \ndef pipeline_1(df):\n    print('current_running_pipeline_1: ')\n\n    #checking the vocab length in the dataframe\n    eng_vocab1 =vocab_forming(df,'english')\n    print('total_english_vocab in df: ',len(eng_vocab1))\n\n    hin_vocab1 = vocab_forming(df,'hindi')\n    print('total_english_vocab in df: ',len(hin_vocab1))\n    \n    #filtering the rows using required vocab\n    df_a = df[\n        (df['english'].apply(filtering_rows_using_vocab,args =[selected_english_vocab,0.96]))&\n         (df['hindi'].apply(filtering_rows_using_vocab,args = [selected_hindi_vocab,0.96]))\n         ]\n    print('final_length of df: ',len(df_a))\n    return df_a","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:23:20.694239Z","iopub.execute_input":"2024-09-26T14:23:20.694663Z","iopub.status.idle":"2024-09-26T14:23:20.702197Z","shell.execute_reply.started":"2024-09-26T14:23:20.694623Z","shell.execute_reply":"2024-09-26T14:23:20.700928Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def pipeline_2(df):\n    global tf_lower_and_remove_punct,tf_lower_and_remove_punct_1,make_dataset\n    print('current_running_pipeline_2: ')\n    text_pairs = list(zip(df['english'],df['hindi']))\n    random.seed(42)\n    random.shuffle(text_pairs)\n    \n   # Assigning  data for validation and testing set\n    num_val_samples = int(0.1*len(text_pairs))\n    num_test_samples = int(0.05*len(text_pairs))\n    num_train_samples = len(text_pairs) -  num_val_samples - num_test_samples\n    train_pairs = text_pairs[:num_train_samples]\n    val_pairs = text_pairs[num_train_samples:num_train_samples+num_val_samples]\n    test_pairs = text_pairs[num_train_samples+num_val_samples:]\n\n    vocab_size_1 = 15000\n    vocab_size_2 = 15000\n    sequence_length = 25\n\n    english_vectorization = tf.keras.layers.TextVectorization(\n        max_tokens = vocab_size_1,\n        output_mode='int',\n        output_sequence_length=sequence_length,\n    #     ragged = True,\n        standardize = tf_lower_and_remove_punct\n\n    )\n\n    hindi_vectorization = tf.keras.layers.TextVectorization(\n        max_tokens = vocab_size_2,\n        output_mode='int',\n        output_sequence_length=sequence_length+1,\n    #     ragged = True,\n        standardize = tf_lower_and_remove_punct_1\n    )\n\n    eng_text = [pair[0] for pair in train_pairs]\n    hin_text = [pair[1] for pair in train_pairs]\n    english_vectorization.adapt(eng_text)\n    hindi_vectorization.adapt(hin_text)\n\n    english_vocab = english_vectorization.get_vocabulary()\n    hindi_vocab = hindi_vectorization.get_vocabulary()\n    vocab_size_eng = len(english_vocab)\n    vocab_size_hin = len(hindi_vocab)\n    print('english_vocab_length: ',vocab_size_eng)\n    print('hindi_vocab_length: ',vocab_size_hin)\n    \n\n    batch_size = 64\n    def format_dataset(eng,hin):\n        eng = english_vectorization(eng)\n        hin = hindi_vectorization(hin)\n        tar_in = hin[:,:-1]\n        tar_out = hin[:,1:]\n        return (eng,tar_in),tar_out\n\n    def make_dataset(pairs):\n        eng_texts, hin_texts = zip(*pairs)\n        eng_texts = list(eng_texts)\n        hin_texts = list(hin_texts)\n        dataset = tf.data.Dataset.from_tensor_slices((eng_texts, hin_texts))\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.map(format_dataset)\n        return dataset\n\n    train_ds = make_dataset(train_pairs)\n    val_ds = make_dataset(val_pairs)\n    test_ds = make_dataset(test_pairs)\n\n    return (train_ds,val_ds,test_ds,english_vectorization,hindi_vectorization,\ntrain_pairs,val_pairs,test_pairs,vocab_size_eng,vocab_size_hin)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:23:21.514219Z","iopub.execute_input":"2024-09-26T14:23:21.514656Z","iopub.status.idle":"2024-09-26T14:23:21.529704Z","shell.execute_reply.started":"2024-09-26T14:23:21.514614Z","shell.execute_reply":"2024-09-26T14:23:21.528743Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def main_pipeline(df):\n    temp_df = pipeline_0(df)\n    temp_df = pipeline_1(temp_df)\n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:23:22.051264Z","iopub.execute_input":"2024-09-26T14:23:22.051724Z","iopub.status.idle":"2024-09-26T14:23:22.056641Z","shell.execute_reply.started":"2024-09-26T14:23:22.051682Z","shell.execute_reply":"2024-09-26T14:23:22.055407Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#df_list for looping\ndf_list = [df3,df4,df5,df6]\n#temporary list to prevent changes in orignal list\ntemp_df_list =[]\nj=1\n#loop to apply pipeline for every df in df_list\nfor i in df_list:\n    print('current_df_running: ',j)\n    print()\n    temp_df = main_pipeline(i)\n    print('final_english_vocab in df: ',len(vocab_forming(temp_df,'english')))\n    print('final_hindi_vocab in df: ',len(vocab_forming(temp_df,'hindi')))\n    temp_df_list.append(temp_df)\n    j+=1\n    print()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:23:29.050579Z","iopub.execute_input":"2024-09-26T14:23:29.051022Z","iopub.status.idle":"2024-09-26T14:26:28.637621Z","shell.execute_reply.started":"2024-09-26T14:23:29.050973Z","shell.execute_reply":"2024-09-26T14:26:28.636520Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"current_df_running:  1\n\ncurrent_running_pipeline_0:\ninitial length of the dataframe:  1265714\nlength of df after proper_cleaning:  735134\ncurrent_running_pipeline_1: \ntotal_english_vocab in df:  116738\ntotal_english_vocab in df:  177180\nfinal_length of df:  57335\nfinal_english_vocab in df:  5728\nfinal_hindi_vocab in df:  6393\n\ncurrent_df_running:  2\n\ncurrent_running_pipeline_0:\ninitial length of the dataframe:  130476\nlength of df after proper_cleaning:  85633\ncurrent_running_pipeline_1: \ntotal_english_vocab in df:  41181\ntotal_english_vocab in df:  44466\nfinal_length of df:  12755\nfinal_english_vocab in df:  3655\nfinal_hindi_vocab in df:  4075\n\ncurrent_df_running:  3\n\ncurrent_running_pipeline_0:\ninitial length of the dataframe:  177606\nlength of df after proper_cleaning:  123046\ncurrent_running_pipeline_1: \ntotal_english_vocab in df:  53033\ntotal_english_vocab in df:  62420\nfinal_length of df:  11035\nfinal_english_vocab in df:  3172\nfinal_hindi_vocab in df:  3274\n\ncurrent_df_running:  4\n\ncurrent_running_pipeline_0:\ninitial length of the dataframe:  1561841\nlength of df after proper_cleaning:  1041474\ncurrent_running_pipeline_1: \ntotal_english_vocab in df:  128117\ntotal_english_vocab in df:  215227\nfinal_length of df:  175974\nfinal_english_vocab in df:  10215\nfinal_hindi_vocab in df:  11463\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#loop to destroy duplicates and stored in new temp_df_list1\ntemp_df_list1 = []\nfor i in range(4):\n    temp_df = temp_df_list[i].drop_duplicates(subset=['english'])\n    temp_df = temp_df.drop_duplicates(subset = ['hindi'])\n    temp_df = temp_df[['english','hindi']]\n    temp_df_list1.append(temp_df)\ndf1 = df1[['english','hindi']]\ndf2 = df2[['english','hindi']]","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:32.373130Z","iopub.execute_input":"2024-09-26T14:26:32.373533Z","iopub.status.idle":"2024-09-26T14:26:32.504283Z","shell.execute_reply.started":"2024-09-26T14:26:32.373494Z","shell.execute_reply":"2024-09-26T14:26:32.503032Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#checking the lengths of dataframe\nfor i in temp_df_list1:\n    print(len(i))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:35.309830Z","iopub.execute_input":"2024-09-26T14:26:35.310231Z","iopub.status.idle":"2024-09-26T14:26:35.316116Z","shell.execute_reply.started":"2024-09-26T14:26:35.310194Z","shell.execute_reply":"2024-09-26T14:26:35.314960Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"50255\n10078\n8601\n77503\n","output_type":"stream"}]},{"cell_type":"code","source":"print('total_english_vocab_in the temp_df_list: ',len(vocab_forming(pd.concat(temp_df_list1),'english')))\nprint('total_hindi_vocab_in the temp_df_list: ',len(vocab_forming(pd.concat(temp_df_list1),'hindi')))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:36.302062Z","iopub.execute_input":"2024-09-26T14:26:36.303074Z","iopub.status.idle":"2024-09-26T14:26:36.976662Z","shell.execute_reply.started":"2024-09-26T14:26:36.303029Z","shell.execute_reply":"2024-09-26T14:26:36.975528Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"total_english_vocab_in the temp_df_list:  9986\ntotal_hindi_vocab_in the temp_df_list:  11512\n","output_type":"stream"}]},{"cell_type":"code","source":"# #this is to sample df if df is too long but not in our case\n# temp_df_list2=[]\n# for i in temp_df_list1:\n#     sample_len=50000\n#     if len(i)<sample_len:\n#         sample_len =len(i)\n#     if len(i)>100000:\n#         sample_len = len(i)//2\n# #     print('current_sample_len: ,'sample_len)\n#     temp_df = i.sample(n = sample_len,random_state =42)\n# #     temp_df = pd.concat([temp_df,temp_df.sample(n = int(0.1*len(temp_df)),random_state = 42)])\n# #     print('total_df_length: ',len(temp_df))\n#     temp_df_list2.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:38.621333Z","iopub.execute_input":"2024-09-26T14:26:38.621756Z","iopub.status.idle":"2024-09-26T14:26:38.627000Z","shell.execute_reply.started":"2024-09-26T14:26:38.621719Z","shell.execute_reply":"2024-09-26T14:26:38.625793Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#concating all df's in temp_df_list1\ndf_a =pd.concat([*temp_df_list1])\n#shuffling df_a\ndf_a = df_a.sample(n = len(df_a),random_state = 42)\n\nprint('length of temp_df_list:',len(df_a))\nprint('length of unique_elements in temp_df_list:',len(df_a.drop_duplicates()))\n\n\n#concating df_a with df1 and df2 to form final_df\nfinal_df = pd.concat([df_a,*[df1]*2,\n                      *[df2,df2.sample(n = int(0.3*len(df2)),random_state = 42)]])\nprint('final_df_length: ',len(final_df))\nprint('unique elements: ',len(final_df.drop_duplicates()))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:40.763153Z","iopub.execute_input":"2024-09-26T14:26:40.763615Z","iopub.status.idle":"2024-09-26T14:26:41.177372Z","shell.execute_reply.started":"2024-09-26T14:26:40.763559Z","shell.execute_reply":"2024-09-26T14:26:41.176388Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"length of temp_df_list: 146437\nlength of unique_elements in temp_df_list: 121015\nfinal_df_length:  181602\nunique elements:  127466\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(set(vocab_forming(final_df,'english'))))\nprint(len(set(vocab_forming(final_df,'hindi'))))\nprint(len(set(vocab_forming(final_df,'english'))&selected_english_vocab))\nprint(len(set(vocab_forming(final_df,'hindi'))&selected_hindi_vocab))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:42.231208Z","iopub.execute_input":"2024-09-26T14:26:42.231719Z","iopub.status.idle":"2024-09-26T14:26:43.732083Z","shell.execute_reply.started":"2024-09-26T14:26:42.231668Z","shell.execute_reply":"2024-09-26T14:26:43.730914Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"10548\n11794\n10511\n11645\n","output_type":"stream"}]},{"cell_type":"code","source":"(train_ds,val_ds,test_ds,english_vectorization,hindi_vectorization,\ntrain_pairs,val_pairs,test_pairs,vocab_size_eng,vocab_size_hin) = pipeline_2(final_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:44.966229Z","iopub.execute_input":"2024-09-26T14:26:44.966904Z","iopub.status.idle":"2024-09-26T14:26:50.555671Z","shell.execute_reply.started":"2024-09-26T14:26:44.966862Z","shell.execute_reply":"2024-09-26T14:26:50.554760Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"current_running_pipeline_2: \nenglish_vocab_length:  10438\nhindi_vocab_length:  11697\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(train_ds))\nprint(len(val_ds))\nprint(len(test_ds))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:50.557583Z","iopub.execute_input":"2024-09-26T14:26:50.558255Z","iopub.status.idle":"2024-09-26T14:26:50.565393Z","shell.execute_reply.started":"2024-09-26T14:26:50.558211Z","shell.execute_reply":"2024-09-26T14:26:50.564397Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"2412\n284\n142\n","output_type":"stream"}]},{"cell_type":"code","source":"#customized val_pairs\nbatch_size = 64\ndef format_dataset(eng,hin):\n    eng = english_vectorization(eng)\n    hin = hindi_vectorization(hin)\n    tar_in = hin[:,:-1]\n    tar_out = hin[:,1:]\n    return (eng,tar_in),tar_out\n\ndef make_dataset(pairs):\n    eng_texts, hin_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    hin_texts = list(hin_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, hin_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset\n\n#both df1 and df2 are main english_sentence_pair that need to be trained so they are present in val_pairs to \n# increase the fitting in train_data\nnew_val_pairs = val_pairs+list(zip(df1['english'],df1['hindi']))+list(zip(df2['english'],df2['hindi']))\nnew_val_pairs = list(set(new_val_pairs))\nprint(len(new_val_pairs))\nnew_val_ds = make_dataset(new_val_pairs)\nprint(len(new_val_ds))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:52.030111Z","iopub.execute_input":"2024-09-26T14:26:52.031039Z","iopub.status.idle":"2024-09-26T14:26:52.627712Z","shell.execute_reply.started":"2024-09-26T14:26:52.030996Z","shell.execute_reply":"2024-09-26T14:26:52.626586Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"38770\n606\n","output_type":"stream"}]},{"cell_type":"code","source":"my_string = \"\"\nfor i,j in new_val_pairs:\n    my_string += i+\"\\t\"+j+\"\\n\"\nwith open(\"customized_val_pairs.txt\",'w',encoding = 'utf-8') as f:\n    f.writelines(my_string)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:57.345446Z","iopub.execute_input":"2024-09-26T14:26:57.346414Z","iopub.status.idle":"2024-09-26T14:26:57.636473Z","shell.execute_reply.started":"2024-09-26T14:26:57.346372Z","shell.execute_reply":"2024-09-26T14:26:57.635604Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"for (inp,tar_in),tar_out in train_ds.take(1):\n    pass\nprint(inp.shape)\nprint(tar_in.shape)\nprint(tar_out.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:02.442748Z","iopub.execute_input":"2024-09-26T14:27:02.443133Z","iopub.status.idle":"2024-09-26T14:27:02.615462Z","shell.execute_reply.started":"2024-09-26T14:27:02.443098Z","shell.execute_reply":"2024-09-26T14:27:02.614389Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"(64, 25)\n(64, 25)\n(64, 25)\n","output_type":"stream"}]},{"cell_type":"code","source":"#positional encoding\ndef positional_encoding(positions,d_model):\n    position = np.arange(positions)[:,np.newaxis]\n    k = np.arange(d_model)[np.newaxis,:]\n    i = k//2\n    angle_rates = 1/(np.power(10000,(2*i)/np.float32(d_model)))\n    angle_rads = position*angle_rates\n#     print('looks of anglerads,: ',angle_rads.shape)\n    angle_rads[:,0::2] = np.sin(angle_rads[:,0::2])\n    angle_rads[:,1::2] = np.cos(angle_rads[:,1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n#     print('new axis angle_rads: ',pos_encoding.shape)\n    return tf.cast(pos_encoding,dtype = tf.float32)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:03.985037Z","iopub.execute_input":"2024-09-26T14:27:03.985436Z","iopub.status.idle":"2024-09-26T14:27:03.993242Z","shell.execute_reply.started":"2024-09-26T14:27:03.985398Z","shell.execute_reply":"2024-09-26T14:27:03.992251Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self,vocab_size,d_model,mask_zero=True):\n        super().__init__()\n        self.d_model = d_model\n        self.mask_zero= mask_zero\n        self.token_embedding = tf.keras.layers.Embedding(input_dim = vocab_size,output_dim= d_model,mask_zero=mask_zero)\n        self.pos_encoding = positional_encoding(128,d_model)\n    \n    def call(self, x):\n        length = tf.shape(x)[1]\n        x = self.token_embedding(x)\n        # This factor sets the relative scale of the embedding and positonal_encoding.\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x = x + self.pos_encoding[:, :length, :]\n        return x\n    def compute_mask(self,inputs,mask =None):\n        if self.mask_zero:\n            return tf.not_equal(inputs,0)\n        else:\n            return None","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:04.811974Z","iopub.execute_input":"2024-09-26T14:27:04.812343Z","iopub.status.idle":"2024-09-26T14:27:04.821618Z","shell.execute_reply.started":"2024-09-26T14:27:04.812309Z","shell.execute_reply":"2024-09-26T14:27:04.820231Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def FullyConnected(embedding_dim,dense_dim):\n    feedforward = tf.keras.Sequential([\n        tf.keras.layers.Dense(dense_dim,activation='relu'),\n        tf.keras.layers.Dense(embedding_dim)\n    ])\n    return feedforward","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:05.430226Z","iopub.execute_input":"2024-09-26T14:27:05.430690Z","iopub.status.idle":"2024-09-26T14:27:05.436700Z","shell.execute_reply.started":"2024-09-26T14:27:05.430649Z","shell.execute_reply":"2024-09-26T14:27:05.435631Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#Encoder Layer\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self,embedding_dim,num_heads,dense_dim,dropout_rate =0.1,layernorm_eps =1e-6,**kwargs):\n        super().__init__(**kwargs)\n\n        self.mha = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim = embedding_dim,\n            dropout = dropout_rate\n        )\n\n        self.ffn = FullyConnected(embedding_dim=embedding_dim,dense_dim=dense_dim)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n        self.supports_masking = True\n    def call(self,x,mask=None):\n        # print('current_mask is: ',mask)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:,None,:],dtype = tf.int32)\n        else:\n            padding_mask = None\n#         print('current_padding_mask: ',padding_mask)\n        attn_output = self.mha( x, x, x,attention_mask = padding_mask)\n        layernorm1_output = self.layernorm1(x + attn_output)\n        feedforward_output = self.ffn(layernorm1_output)\n        dropout_ffn_output = self.dropout_ffn(feedforward_output)\n        encoder_layer_output = self.layernorm2(layernorm1_output + dropout_ffn_output)\n\n        return encoder_layer_output","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:05.950343Z","iopub.execute_input":"2024-09-26T14:27:05.950751Z","iopub.status.idle":"2024-09-26T14:27:05.961133Z","shell.execute_reply.started":"2024-09-26T14:27:05.950712Z","shell.execute_reply":"2024-09-26T14:27:05.960172Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self,embedding_dim,num_heads,dense_dim,num_layers,input_vocab_size,\n                 dropout_rate =0.1,layernorm_eps =1e-6):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n\n        self.embedding = PositionalEmbedding(input_vocab_size,self.embedding_dim)\n        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n                                        num_heads=num_heads,\n                                        dense_dim=dense_dim,\n                                        dropout_rate=dropout_rate,\n                                        layernorm_eps=layernorm_eps) \n                           for _ in range(self.num_layers)]\n        \n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.supports_masking =True\n    def call(self,x,mask =None):\n        # print('current mask: ',mask)\n        x = self.embedding(x)\n        x = self.dropout(x)\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:06.530187Z","iopub.execute_input":"2024-09-26T14:27:06.530603Z","iopub.status.idle":"2024-09-26T14:27:06.539732Z","shell.execute_reply.started":"2024-09-26T14:27:06.530567Z","shell.execute_reply":"2024-09-26T14:27:06.538680Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self,embedding_dim,num_heads,dense_dim,dropout_rate =0.1,layernorm_eps =1e-6):\n        super().__init__()\n        self.mha1 = tf.keras.layers.MultiHeadAttention(key_dim=embedding_dim,num_heads = num_heads,dropout=dropout_rate)\n        self.mha2 = tf.keras.layers.MultiHeadAttention(key_dim = embedding_dim,num_heads = num_heads,dropout =dropout_rate)\n        self.ffn = FullyConnected(embedding_dim=embedding_dim,dense_dim=dense_dim)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.supports_masking = True\n    def call(self,x,enc_output,mask=None):\n        casual_mask = self.get_casual_attention_mask(x)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:,None,:],dtype = tf.int32)\n#             padding_mask = tf.minimum(padding_mask,casual_mask)\n        else:\n            padding_mask = None\n\n        attn_out1 ,attn_scores = self.mha1(x,x,x,attention_mask=casual_mask,\n                                           return_attention_scores = True)\n        self.last_attn_scores = attn_scores\n        Q = self.layernorm1(x+attn_out1)\n\n        attn_out2 = self.mha2(query=Q,key=enc_output,value=enc_output,\n                             attention_mask = padding_mask)\n        attn_out2 = self.layernorm2(Q+attn_out2)\n\n        ffn_output = self.ffn(attn_out2)\n        ffn_output = self.dropout(ffn_output)\n        decoder_output = self.layernorm3(attn_out2+ffn_output)\n        \n        return decoder_output\n\n    def get_casual_attention_mask(self,x):\n        input_shape = tf.shape(x)\n        batch_size,sequence_length = input_shape[0],input_shape[1]\n        i = tf.range(sequence_length)[:,None]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i>=j,tf.int32)\n        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1]))\n        mult = tf.concat(\n            [\n                tf.expand_dims(batch_size,-1),\n                tf.convert_to_tensor([1,1]),\n            ],\n            axis =0,\n        )\n        return tf.tile(mask,mult)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:06.843876Z","iopub.execute_input":"2024-09-26T14:27:06.844314Z","iopub.status.idle":"2024-09-26T14:27:06.861007Z","shell.execute_reply.started":"2024-09-26T14:27:06.844270Z","shell.execute_reply":"2024-09-26T14:27:06.859627Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self,embedding_dim,num_heads,dense_dim,num_layers,output_vocab_size,dropout_rate=0.1,layernorm_eps=1e-6):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n\n        self.embedding = PositionalEmbedding(output_vocab_size,self.embedding_dim)\n        self.dec_layer = [DecoderLayer(embedding_dim=embedding_dim,\n                               num_heads=num_heads,\n                               dense_dim=dense_dim,\n                               dropout_rate=dropout_rate,\n                               layernorm_eps=layernorm_eps) \n                        for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self,x,enc_output):\n        attention_weights = {}\n        x = self.embedding(x)\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x = self.dec_layer[i](x,enc_output)\n        #update attention_weights dictionary with the attention weights of block 1 and block 2\n            # attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n            # attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:07.150247Z","iopub.execute_input":"2024-09-26T14:27:07.150670Z","iopub.status.idle":"2024-09-26T14:27:07.160687Z","shell.execute_reply.started":"2024-09-26T14:27:07.150631Z","shell.execute_reply":"2024-09-26T14:27:07.159589Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, embedding_dim, num_heads,dense_dim, num_layers,input_vocab_size, \n               output_vocab_size, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers=num_layers,\n                               embedding_dim=embedding_dim,\n                               num_heads=num_heads,\n                               dense_dim=dense_dim,\n                               input_vocab_size=input_vocab_size,\n                               dropout_rate=dropout_rate,\n                               layernorm_eps=layernorm_eps)\n\n        self.decoder = Decoder(num_layers=num_layers, \n                               embedding_dim=embedding_dim,\n                               num_heads=num_heads,\n                               dense_dim=dense_dim,\n                               output_vocab_size=output_vocab_size, \n                               dropout_rate=dropout_rate,\n                               layernorm_eps=layernorm_eps)\n\n        self.final_layer = tf.keras.layers.Dense(output_vocab_size, activation=tf.nn.log_softmax)\n    \n    def call(self, inputs):\n        # print(type(inputs))\n        input_sentence,output_sentence = inputs\n        # print('inputs okay')\n        enc_output = self.encoder(input_sentence)\n        # print('enc_output_generated')\n        \n        # call self.decoder with the appropriate arguments to get the decoder output\n        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n        dec_output = self.decoder(output_sentence,enc_output)\n        # print('dec_output_generated')\n        \n        # pass decoder output through a linear layer and softmax (~1 line)\n        logits = self.final_layer(dec_output)\n        ### END CODE HERE ###\n        try:\n        # Drop the keras mask, so it doesn't scale the losses/metrics.\n        # b/250038731\n            del logits._keras_mask\n        except AttributeError:\n            pass\n\n        # Return the final output and the attention weights.\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:07.469381Z","iopub.execute_input":"2024-09-26T14:27:07.470169Z","iopub.status.idle":"2024-09-26T14:27:07.480259Z","shell.execute_reply.started":"2024-09-26T14:27:07.470128Z","shell.execute_reply":"2024-09-26T14:27:07.479205Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"embed_dim = 128\nheads = 2\nlatent = 512\nno_of_layers = 2\n\ntransformer1 = Transformer(embedding_dim=embed_dim,num_heads=heads,dense_dim=latent,\n                          num_layers=no_of_layers,input_vocab_size=vocab_size_eng,\n                           output_vocab_size=vocab_size_hin)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:07.930322Z","iopub.execute_input":"2024-09-26T14:27:07.931274Z","iopub.status.idle":"2024-09-26T14:27:08.011718Z","shell.execute_reply.started":"2024-09-26T14:27:07.931226Z","shell.execute_reply":"2024-09-26T14:27:08.010614Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#optimizer\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super().__init__()\n\n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n\n  def __call__(self, step):\n    step = tf.cast(step, dtype=tf.float32)\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n\n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:10.841837Z","iopub.execute_input":"2024-09-26T14:27:10.842947Z","iopub.status.idle":"2024-09-26T14:27:10.850155Z","shell.execute_reply.started":"2024-09-26T14:27:10.842901Z","shell.execute_reply":"2024-09-26T14:27:10.849153Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"logits = transformer1((inp,tar_in))\ntransformer1.summary()\nlearning_rate = CustomSchedule(d_model=embed_dim)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\ndef masked_loss(y_true, y_pred):\n    \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    loss = loss_fn(y_true, y_pred)\n    \n    # Check which elements of y_true are padding\n    mask = tf.cast(y_true != 0, loss.dtype)\n    \n    loss *= mask\n    # Return the total.\n    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n\n\ndef masked_acc(y_true, y_pred):\n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n    match = tf.cast(y_true == y_pred, tf.float32)\n    mask = tf.cast(y_true != 0, tf.float32)\n    match*= mask\n\n    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:11.395495Z","iopub.execute_input":"2024-09-26T14:27:11.395929Z","iopub.status.idle":"2024-09-26T14:27:16.707720Z","shell.execute_reply.started":"2024-09-26T14:27:11.395890Z","shell.execute_reply":"2024-09-26T14:27:16.706878Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"transformer\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m1,864,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m2,290,048\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │     \u001b[38;5;34m1,508,913\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,864,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,290,048</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,508,913</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,663,409\u001b[0m (21.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,663,409</span> (21.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,663,409\u001b[0m (21.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,663,409</span> (21.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"transformer1.compile(optimizer =optimizer,loss = masked_loss,metrics=[masked_acc])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:27:16.709577Z","iopub.execute_input":"2024-09-26T14:27:16.710398Z","iopub.status.idle":"2024-09-26T14:27:16.720134Z","shell.execute_reply.started":"2024-09-26T14:27:16.710349Z","shell.execute_reply":"2024-09-26T14:27:16.719308Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"epochs =15\n# steps_per_epoch =500\ntransformer1.fit(train_ds,\n                 epochs =epochs,\n#                  steps_per_epoch = steps_per_epoch,\n                 validation_data=new_val_ds,\n#                  validation_steps =20,\n                callbacks = tf.keras.callbacks.EarlyStopping(patience =1)\n                )","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:44:58.543020Z","iopub.execute_input":"2024-09-26T14:44:58.543894Z","iopub.status.idle":"2024-09-26T14:46:28.002180Z","shell.execute_reply.started":"2024-09-26T14:44:58.543850Z","shell.execute_reply":"2024-09-26T14:46:28.001212Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Epoch 1/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 19ms/step - loss: 0.8963 - masked_acc: 0.8194 - val_loss: 1.6543 - val_masked_acc: 0.7450\nEpoch 2/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 18ms/step - loss: 0.8689 - masked_acc: 0.8249 - val_loss: 1.6562 - val_masked_acc: 0.7460\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b42e9da6710>"},"metadata":{}}]},{"cell_type":"code","source":"x1 = transformer1.evaluate(train_ds)\nprint(x1)\nx2 = transformer1.evaluate(val_ds)\nprint(x2)\nx3 = transformer1.evaluate(test_ds)\nprint(x3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:46:38.429649Z","iopub.execute_input":"2024-09-26T14:46:38.430046Z","iopub.status.idle":"2024-09-26T14:46:54.934699Z","shell.execute_reply.started":"2024-09-26T14:46:38.430008Z","shell.execute_reply":"2024-09-26T14:46:54.933629Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - loss: 0.9344 - masked_acc: 0.8001\n[0.8272944688796997, 0.8199390769004822]\n\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.2357 - masked_acc: 0.6403\n[2.2312262058258057, 0.6417375206947327]\n\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2844 - masked_acc: 0.6377\n[2.287414073944092, 0.6375607252120972]\n","output_type":"stream"}]},{"cell_type":"code","source":"transformer1.evaluate(new_val_ds)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:47:36.331047Z","iopub.execute_input":"2024-09-26T14:47:36.331923Z","iopub.status.idle":"2024-09-26T14:47:39.917507Z","shell.execute_reply.started":"2024-09-26T14:47:36.331866Z","shell.execute_reply":"2024-09-26T14:47:39.916526Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"\u001b[1m606/606\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1.6469 - masked_acc: 0.7463\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"[1.656206727027893, 0.7460039854049683]"},"metadata":{}}]},{"cell_type":"code","source":"#save the dataframe\nfinal_df.to_csv('english_hindi_10k_vocab.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:47:46.530672Z","iopub.execute_input":"2024-09-26T14:47:46.531060Z","iopub.status.idle":"2024-09-26T14:47:47.410558Z","shell.execute_reply.started":"2024-09-26T14:47:46.531025Z","shell.execute_reply":"2024-09-26T14:47:47.409596Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#save the vectorization layers\nenglish_vectorization_config = english_vectorization.get_config()\nenglish_vectorization_config.pop('standardize', None)\nenglish_vocab = english_vectorization.get_vocabulary()\nwith open('english_vectorization_config.json', 'w', encoding='utf-8') as f:\n    json.dump(english_vectorization_config, f)\n    \nwith open('english_vocab.json', 'w', encoding='utf-8') as f:\n    json.dump(english_vocab, f)\n    \nhindi_vectorization_config = hindi_vectorization.get_config()\nhindi_vectorization_config.pop('standardize', None)\nhindi_vocab = hindi_vectorization.get_vocabulary()\nwith open('hindi_vectorization_config.json', 'w', encoding='utf-8') as f:\n    json.dump(hindi_vectorization_config, f)\n    \nwith open('hindi_vocab.json', 'w', encoding='utf-8') as f:\n    json.dump(hindi_vocab, f)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:47:48.829493Z","iopub.execute_input":"2024-09-26T14:47:48.830047Z","iopub.status.idle":"2024-09-26T14:47:48.959606Z","shell.execute_reply.started":"2024-09-26T14:47:48.830003Z","shell.execute_reply":"2024-09-26T14:47:48.958711Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#save the model weights\ntransformer1.save_weights('english_hindi_model.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:47:50.840280Z","iopub.execute_input":"2024-09-26T14:47:50.841133Z","iopub.status.idle":"2024-09-26T14:47:51.184955Z","shell.execute_reply.started":"2024-09-26T14:47:50.841088Z","shell.execute_reply":"2024-09-26T14:47:51.183658Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"hindi_vocab = hindi_vectorization.get_vocabulary()\nhindi_index_lookup = dict(zip(range(len(hindi_vocab)), hindi_vocab))\nmax_decoded_sentence_length = 25\n\ndef decode_sentence(input_sentence):\n    tokenized_input_sentence = english_vectorization([input_sentence])\n    decoded_sentence = \"[SOS]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = hindi_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer1([tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = tf.argmax(predictions[0, i, :]).numpy().item(0)\n        sampled_token = hindi_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[EOS]\":\n            break\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:47:54.230246Z","iopub.execute_input":"2024-09-26T14:47:54.230994Z","iopub.status.idle":"2024-09-26T14:47:54.297865Z","shell.execute_reply.started":"2024-09-26T14:47:54.230950Z","shell.execute_reply":"2024-09-26T14:47:54.296960Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"random.seed(42)\nfor _ in range(5):\n    input_sentence,output_sentence = random.choice(train_pairs)\n    # input_sentence = input_sentence.lower()\n    # input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n    translated = decode_sentence(input_sentence)\n    print(f\"input: {input_sentence}\")\n    print(f\"translated: {translated}\")\n    print(f\"original  : [SOS] {output_sentence} [EOS]\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:47:56.296288Z","iopub.execute_input":"2024-09-26T14:47:56.296723Z","iopub.status.idle":"2024-09-26T14:48:02.032522Z","shell.execute_reply.started":"2024-09-26T14:47:56.296682Z","shell.execute_reply":"2024-09-26T14:48:02.031390Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"input: initial file chooser folder\ntranslated: [SOS] आरंभिक फ़ाइल चयनक फ़ोल्डर [EOS]\noriginal  : [SOS] प्रारंभिक फ़ाइल चयनकर्ता फ़ोल्डर [EOS]\n\ninput: so the first thing to figure out about this hyperbola is ,\ntranslated: [SOS] तो , पहली बात यह है के बारे में इस अति परवलय समझ से बाहर है [EOS]\noriginal  : [SOS] तो , पहली बात यह है के बारे में इस अति परवलय समझ से बाहर है [EOS]\n\ninput: . what does right to information mean ?\ntranslated: [SOS] सूचना अधिकार का क्या अर्थ है ? [EOS]\noriginal  : [SOS] . सूचना अधिकार का क्या अर्थ है ? [EOS]\n\ninput: but i don t think that it s strange at all .\ntranslated: [SOS] लेकिन मुझे लगता है कि यह सब अजीब है [EOS]\noriginal  : [SOS] पर मेरे खयाल से यह बिलकुल भी अजीब नहीं है । [EOS]\n\ninput: that is also a positive .\ntranslated: [SOS] यह भी सकारात्मक है । [EOS]\noriginal  : [SOS] वह भी सकारात्मक अर्थ में . [EOS]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"random.seed(42)\nfor _ in range(5):\n    input_sentence,output_sentence = random.choice(val_pairs)\n    # input_sentence = input_sentence.lower()\n    # input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n    translated = decode_sentence(input_sentence)\n    print(f\"input: {input_sentence}\")\n    print(f\"translated: {translated}\")\n    print(f\"original  : [SOS] {output_sentence} [EOS]\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:48:14.802284Z","iopub.execute_input":"2024-09-26T14:48:14.803490Z","iopub.status.idle":"2024-09-26T14:48:19.643604Z","shell.execute_reply.started":"2024-09-26T14:48:14.803442Z","shell.execute_reply":"2024-09-26T14:48:19.642592Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"input: and carry the .\ntranslated: [SOS] हासिल हुआ [EOS]\noriginal  : [SOS] और को हासिल ले लेते हैं [EOS]\n\ninput: india is a country in south asia .\ntranslated: [SOS] भारत दक्षिण एशिया का एक देश है । [EOS]\noriginal  : [SOS] भारत देश उत्तरी गोलार्ध में स्थित है । [EOS]\n\ninput: we should first make sure that it s not in this world .\ntranslated: [SOS] जो हम दुनिया को यकीन न करें वह इस संसार में नहीं जानता । [EOS]\noriginal  : [SOS] हमे पहले यह निश्चित करना चाहिए कि यह इस दुनिया का नहीं है । [EOS]\n\ninput: job type\ntranslated: [SOS] कार्य प्रकार [EOS]\noriginal  : [SOS] कार्य क़िस्म [EOS]\n\ninput: but we kind of chose this shape .\ntranslated: [SOS] लेकिन हमने इस तरह की आकृति को चुना [EOS]\noriginal  : [SOS] लेकिन हमने इस तरह की आकृति को चुना . [EOS]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"random.seed(42)\nfor _ in range(5):\n    input_sentence,output_sentence = random.choice(test_pairs)\n    # input_sentence = input_sentence.lower()\n    # input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n    translated = decode_sentence(input_sentence)\n    print(f\"input: {input_sentence}\")\n    print(f\"translated: {translated}\")\n    print(f\"original  : [SOS] {output_sentence} [EOS]\")\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:48:27.291016Z","iopub.execute_input":"2024-09-26T14:48:27.292012Z","iopub.status.idle":"2024-09-26T14:48:32.034951Z","shell.execute_reply.started":"2024-09-26T14:48:27.291964Z","shell.execute_reply":"2024-09-26T14:48:32.033647Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"input: so we wanted to win this match .\ntranslated: [SOS] इसलिए हम इस मैच को जीतना चाहते हैं । [EOS]\noriginal  : [SOS] इसलिए हम इस मैच को जीतना चाहते थे . [EOS]\n\ninput: it will have a common name all over the country .\ntranslated: [SOS] यह एक आम नाम के सभी लोग ही हैं । [EOS]\noriginal  : [SOS] देश भर में दुकान के लिए एक ही नाम होगा । [EOS]\n\ninput: please select a folder below\ntranslated: [SOS] कृपया नीचे कोई फ़ोल्डर चुनें [EOS]\noriginal  : [SOS] कृपया एक फोल्डर नीचे से चुनें [EOS]\n\ninput: and second of all ,\ntranslated: [SOS] और दूसरी बात , [EOS]\noriginal  : [SOS] और दूसरा , [EOS]\n\ninput: end of a short break\ntranslated: [SOS] छोटे ब्रेक की समाप्ति [EOS]\noriginal  : [SOS] छोटे ब्रेक की समाप्ति [EOS]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"my_text = [\"where are you ?\",\n          \"i am not one of them \",\n          \"what do you think about them ?\",\n          \"today is a good day\",\n          \"who are you?\",]","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:48:34.900075Z","iopub.execute_input":"2024-09-26T14:48:34.900655Z","iopub.status.idle":"2024-09-26T14:48:34.906591Z","shell.execute_reply.started":"2024-09-26T14:48:34.900581Z","shell.execute_reply":"2024-09-26T14:48:34.905116Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"for i in my_text:\n    print('sentence: ',i)\n    print('translation: ',decode_sentence(i))\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:48:38.262169Z","iopub.execute_input":"2024-09-26T14:48:38.263018Z","iopub.status.idle":"2024-09-26T14:48:42.095014Z","shell.execute_reply.started":"2024-09-26T14:48:38.262974Z","shell.execute_reply":"2024-09-26T14:48:42.093998Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"sentence:  where are you ?\ntranslation:  [SOS] तुम कहाँ हो ? [EOS]\n\nsentence:  i am not one of them \ntranslation:  [SOS] मैं उनमें से एक नहीं हूं [EOS]\n\nsentence:  what do you think about them ?\ntranslation:  [SOS] क्या आपको क्या लगता है ? [EOS]\n\nsentence:  today is a good day\ntranslation:  [SOS] आज के दिन अच्छा है । [EOS]\n\nsentence:  who are you?\ntranslation:  [SOS] तुम कौन हो ? [EOS]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Running the transfomer without masked accuracy and masked loss","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:48:52.814249Z","iopub.execute_input":"2024-09-26T14:48:52.815255Z","iopub.status.idle":"2024-09-26T14:48:52.819642Z","shell.execute_reply.started":"2024-09-26T14:48:52.815208Z","shell.execute_reply":"2024-09-26T14:48:52.818431Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#simpler model\nembed_dim2 = 128\nheads2 = 2\nlatent2 = 512\nno_of_layers2 = 2\ntransformer2 = Transformer(embedding_dim=embed_dim2,num_heads=heads2,dense_dim=latent2,\n                          num_layers=no_of_layers2,input_vocab_size=vocab_size_eng,\n                           output_vocab_size=vocab_size_hin)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:48:54.079298Z","iopub.execute_input":"2024-09-26T14:48:54.080079Z","iopub.status.idle":"2024-09-26T14:48:54.154415Z","shell.execute_reply.started":"2024-09-26T14:48:54.080032Z","shell.execute_reply":"2024-09-26T14:48:54.153382Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"transformer2((inp,tar_in))\ntransformer2.summary()\nnormal_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntransformer2.compile(optimizer = 'adam',loss = normal_loss,metrics = ['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:48:56.920696Z","iopub.execute_input":"2024-09-26T14:48:56.921765Z","iopub.status.idle":"2024-09-26T14:48:59.879717Z","shell.execute_reply.started":"2024-09-26T14:48:56.921716Z","shell.execute_reply":"2024-09-26T14:48:59.878821Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"transformer_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m1,864,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m2,290,048\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │     \u001b[38;5;34m1,508,913\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,864,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,290,048</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,508,913</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,663,409\u001b[0m (21.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,663,409</span> (21.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,663,409\u001b[0m (21.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,663,409</span> (21.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"epochs =15\n# steps_per_epoch =500\ntransformer2.fit(train_ds,\n                 epochs =epochs,\n#                  steps_per_epoch = steps_per_epoch,\n                 validation_data=new_val_ds,\n#                  validation_steps =20,\n                callbacks = tf.keras.callbacks.EarlyStopping(patience =3)\n                )","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:49:17.241685Z","iopub.execute_input":"2024-09-26T14:49:17.242138Z","iopub.status.idle":"2024-09-26T14:59:01.360293Z","shell.execute_reply.started":"2024-09-26T14:49:17.242087Z","shell.execute_reply":"2024-09-26T14:59:01.359186Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1727362172.497338     111 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2410/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7997 - loss: 1.6050","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1727362221.579760     113 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7998 - loss: 1.6046","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1727362228.400420     111 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nW0000 00:00:1727362233.384636     111 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 24ms/step - accuracy: 0.7998 - loss: 1.6044 - val_accuracy: 0.9045 - val_loss: 0.5390\nEpoch 2/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 19ms/step - accuracy: 0.8730 - loss: 0.6524 - val_accuracy: 0.9237 - val_loss: 0.3794\nEpoch 3/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 19ms/step - accuracy: 0.8922 - loss: 0.4992 - val_accuracy: 0.9312 - val_loss: 0.3316\nEpoch 4/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 19ms/step - accuracy: 0.9033 - loss: 0.4275 - val_accuracy: 0.9347 - val_loss: 0.3141\nEpoch 5/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 19ms/step - accuracy: 0.9166 - loss: 0.3509 - val_accuracy: 0.9396 - val_loss: 0.2970\nEpoch 7/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 19ms/step - accuracy: 0.9213 - loss: 0.3257 - val_accuracy: 0.9396 - val_loss: 0.2992\nEpoch 8/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 19ms/step - accuracy: 0.9249 - loss: 0.3065 - val_accuracy: 0.9410 - val_loss: 0.2972\nEpoch 9/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 19ms/step - accuracy: 0.9280 - loss: 0.2901 - val_accuracy: 0.9428 - val_loss: 0.2935\nEpoch 10/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 19ms/step - accuracy: 0.9306 - loss: 0.2770 - val_accuracy: 0.9433 - val_loss: 0.2976\nEpoch 11/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 19ms/step - accuracy: 0.9327 - loss: 0.2653 - val_accuracy: 0.9442 - val_loss: 0.2978\nEpoch 12/15\n\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 20ms/step - accuracy: 0.9349 - loss: 0.2550 - val_accuracy: 0.9449 - val_loss: 0.2975\n","output_type":"stream"},{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b42e24b6920>"},"metadata":{}}]},{"cell_type":"code","source":"y1 = transformer2.evaluate(train_ds)\nprint(y1)\ny2 = transformer2.evaluate(val_ds)\nprint(y2)\ny3 = transformer2.evaluate(test_ds)\nprint(y3)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:00:28.136205Z","iopub.execute_input":"2024-09-26T15:00:28.136635Z","iopub.status.idle":"2024-09-26T15:00:45.975029Z","shell.execute_reply.started":"2024-09-26T15:00:28.136595Z","shell.execute_reply":"2024-09-26T15:00:45.973709Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"\u001b[1m2412/2412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.9311 - loss: 0.2694\n[0.24400301277637482, 0.9367454648017883]\n\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8990 - loss: 0.5667\n[0.5677531957626343, 0.8987598419189453]\n\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9010 - loss: 0.5673\n[0.562375545501709, 0.9008325338363647]\n","output_type":"stream"}]},{"cell_type":"code","source":"transformer2.evaluate(new_val_ds)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:00:58.971366Z","iopub.execute_input":"2024-09-26T15:00:58.972355Z","iopub.status.idle":"2024-09-26T15:01:02.809187Z","shell.execute_reply.started":"2024-09-26T15:00:58.972297Z","shell.execute_reply":"2024-09-26T15:01:02.808255Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"\u001b[1m606/606\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9446 - loss: 0.2985\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"[0.29748889803886414, 0.9448650479316711]"},"metadata":{}}]},{"cell_type":"code","source":"hindi_vocab = hindi_vectorization.get_vocabulary()\nhindi_index_lookup = dict(zip(range(len(hindi_vocab)), hindi_vocab))\nmax_decoded_sentence_length = 25\n\ndef decode_sentence_2(input_sentence):\n    tokenized_input_sentence = english_vectorization([input_sentence])\n    decoded_sentence = \"[SOS]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = hindi_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer2([tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = tf.argmax(predictions[0, i, :]).numpy().item(0)\n        sampled_token = hindi_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[EOS]\":\n            break\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:01:23.230398Z","iopub.execute_input":"2024-09-26T15:01:23.230836Z","iopub.status.idle":"2024-09-26T15:01:23.299902Z","shell.execute_reply.started":"2024-09-26T15:01:23.230793Z","shell.execute_reply":"2024-09-26T15:01:23.298918Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"random.seed(42)\nfor _ in range(5):\n    input_sentence,output_sentence = random.choice(train_pairs)\n    # input_sentence = input_sentence.lower()\n    # input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n    translated = decode_sentence_2(input_sentence)\n    print(f\"input: {input_sentence}\")\n    print(f\"translated: {translated}\")\n    print(f\"original  : [SOS] {output_sentence} [EOS]\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:01:23.707382Z","iopub.execute_input":"2024-09-26T15:01:23.708361Z","iopub.status.idle":"2024-09-26T15:01:29.274411Z","shell.execute_reply.started":"2024-09-26T15:01:23.708317Z","shell.execute_reply":"2024-09-26T15:01:29.273305Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"input: initial file chooser folder\ntranslated: [SOS] आरंभिक फ़ाइल चयनक फ़ोल्डर [EOS]\noriginal  : [SOS] प्रारंभिक फ़ाइल चयनकर्ता फ़ोल्डर [EOS]\n\ninput: so the first thing to figure out about this hyperbola is ,\ntranslated: [SOS] तो पहली बात यह है कि इस अति परवलय समझ से बाहर है [EOS]\noriginal  : [SOS] तो , पहली बात यह है के बारे में इस अति परवलय समझ से बाहर है [EOS]\n\ninput: . what does right to information mean ?\ntranslated: [SOS] जरूरी जानकारी है इसका क्या मतलब है ? [EOS]\noriginal  : [SOS] . सूचना अधिकार का क्या अर्थ है ? [EOS]\n\ninput: but i don t think that it s strange at all .\ntranslated: [SOS] लेकिन मुझे नहीं लगता कि यह अजीब है । [EOS]\noriginal  : [SOS] पर मेरे खयाल से यह बिलकुल भी अजीब नहीं है । [EOS]\n\ninput: that is also a positive .\ntranslated: [SOS] यह भी सकारात्मक है । [EOS]\noriginal  : [SOS] वह भी सकारात्मक अर्थ में . [EOS]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"random.seed(42)\nfor _ in range(5):\n    input_sentence,output_sentence = random.choice(val_pairs)\n    # input_sentence = input_sentence.lower()\n    # input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n    translated = decode_sentence_2(input_sentence)\n    print(f\"input: {input_sentence}\")\n    print(f\"translated: {translated}\")\n    print(f\"original  : [SOS] {output_sentence} [EOS]\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:01:29.276707Z","iopub.execute_input":"2024-09-26T15:01:29.277627Z","iopub.status.idle":"2024-09-26T15:01:34.325422Z","shell.execute_reply.started":"2024-09-26T15:01:29.277571Z","shell.execute_reply":"2024-09-26T15:01:34.324087Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"input: and carry the .\ntranslated: [SOS] और हासिल कर [EOS]\noriginal  : [SOS] और को हासिल ले लेते हैं [EOS]\n\ninput: india is a country in south asia .\ntranslated: [SOS] दक्षिण एशिया का एक देश है । [EOS]\noriginal  : [SOS] भारत देश उत्तरी गोलार्ध में स्थित है । [EOS]\n\ninput: we should first make sure that it s not in this world .\ntranslated: [SOS] हमें यह पूरी दुनिया में यह है कि यह दुनिया में नहीं होना चाहिए [EOS]\noriginal  : [SOS] हमे पहले यह निश्चित करना चाहिए कि यह इस दुनिया का नहीं है । [EOS]\n\ninput: job type\ntranslated: [SOS] कार्य प्रकार [EOS]\noriginal  : [SOS] कार्य क़िस्म [EOS]\n\ninput: but we kind of chose this shape .\ntranslated: [SOS] लेकिन हमने इस तरह का आकार बदल दिया है [EOS]\noriginal  : [SOS] लेकिन हमने इस तरह की आकृति को चुना . [EOS]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"random.seed(42)\nfor _ in range(5):\n    input_sentence,output_sentence = random.choice(test_pairs)\n    # input_sentence = input_sentence.lower()\n    # input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n    translated = decode_sentence_2(input_sentence)\n    print(f\"input: {input_sentence}\")\n    print(f\"translated: {translated}\")\n    print(f\"original  : [SOS] {output_sentence} [EOS]\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T15:01:34.326754Z","iopub.execute_input":"2024-09-26T15:01:34.327094Z","iopub.status.idle":"2024-09-26T15:01:38.819165Z","shell.execute_reply.started":"2024-09-26T15:01:34.327057Z","shell.execute_reply":"2024-09-26T15:01:38.818020Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"input: so we wanted to win this match .\ntranslated: [SOS] तो हम इस मैच को जीतना चाहते हैं । [EOS]\noriginal  : [SOS] इसलिए हम इस मैच को जीतना चाहते थे . [EOS]\n\ninput: it will have a common name all over the country .\ntranslated: [SOS] देश के साथ यह सब कुछ ही नाम होगा [EOS]\noriginal  : [SOS] देश भर में दुकान के लिए एक ही नाम होगा । [EOS]\n\ninput: please select a folder below\ntranslated: [SOS] कृपया निम्न फ़ोल्डर चुनें [EOS]\noriginal  : [SOS] कृपया एक फोल्डर नीचे से चुनें [EOS]\n\ninput: and second of all ,\ntranslated: [SOS] और दूसरी चीज़ें , [EOS]\noriginal  : [SOS] और दूसरा , [EOS]\n\ninput: end of a short break\ntranslated: [SOS] छोटे ब्रेक का अंत [EOS]\noriginal  : [SOS] छोटे ब्रेक की समाप्ति [EOS]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# we can see some differences in masked accuracy translations and normal accuracy translations","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}