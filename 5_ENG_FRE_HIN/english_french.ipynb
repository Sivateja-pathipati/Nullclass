{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/english_french.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We do not have any choice.</td>\n",
       "      <td>Nous n'avons pas le choix.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like that it is soft.</td>\n",
       "      <td>J'aime que ce soit doux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Was there an earthquake?</td>\n",
       "      <td>Y a-t-il eu un tremblement de terre ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They say he is sick.</td>\n",
       "      <td>Elles disent qu'il est malade.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You should always wear a seat belt when you ar...</td>\n",
       "      <td>On devrait toujours mettre une ceinture lorsqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                         We do not have any choice.   \n",
       "1                            I like that it is soft.   \n",
       "2                           Was there an earthquake?   \n",
       "3                               They say he is sick.   \n",
       "4  You should always wear a seat belt when you ar...   \n",
       "\n",
       "                                              French  \n",
       "0                         Nous n'avons pas le choix.  \n",
       "1                           J'aime que ce soit doux.  \n",
       "2              Y a-t-il eu un tremblement de terre ?  \n",
       "3                     Elles disent qu'il est malade.  \n",
       "4  On devrait toujours mettre une ceinture lorsqu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English    False\n",
       "French     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I did not want to give up.', 'Je ne voulais pas abandonner.'), ('What is happening?', \"Qu'est-ce qui se passe\\u202f?\"), ('Protesters tried to disrupt the meeting.', 'Les protestataires tentèrent de perturber la réunion.'), ('You could not have picked a better spot.', \"Vous n'auriez pas pu choisir un meilleur endroit.\"), ('That looks like it hurts.', 'On dirait que ça fait mal.')]\n"
     ]
    }
   ],
   "source": [
    "text_pairs =list(zip(df['English'],df['French']))\n",
    "random.seed(42)\n",
    "random.shuffle(text_pairs)\n",
    "print(text_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108742 total pairs\n",
      "86994 training pairs\n",
      "10874 validation pairs\n",
      "10874 test pairs\n"
     ]
    }
   ],
   "source": [
    "num_val_samples = int(0.1 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) -  2*num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples+num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f'{len(test_pairs)} test pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx english vocab: 16443\n",
      "approx french vocab: 29092\n"
     ]
    }
   ],
   "source": [
    "list1 = list(df['English'])\n",
    "list1 = [word.strip().strip('!').strip('.').strip(',').strip('?').lower() for sentence in list1 for word in sentence.split(' ') if len(word)]\n",
    "\n",
    "print('approx english vocab:',len(set(list1)))\n",
    "\n",
    "list2 = list(df['French'])\n",
    "list2 = [word.strip('!').strip('.').strip(',').strip('?') for sentence in list2 for word in sentence.split(' ')]\n",
    "print('approx french vocab:',len(set(list2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'relatively', 'safe', 'neighborhood', 'tom', 'and', 'mary', 'were', 'on', 'the']\n",
      "['same', 'wavelength', 'kripananda', 'this', 'place', 'is', 'boring', 'what', 'is', 'your']\n",
      "['favorite', 'toothpaste', 'qualitised', 'postulated', 'complement', 'why', 'do', 'not', 'you', 'tell']\n",
      "['me', 'what', 'you', 'want', 'to', 'hear', 'someone', 'cleaned', 'my', 'room']\n",
      "['while', 'i', 'was', 'gone', 'he', 'lived', 'alone', 'in', 'the', 'forest']\n",
      "['i', 'feel', 'bad', 'that', 'i', 'have', 'not', 'paid', 'you', 'yet']\n",
      "['are', 'you', 'already', 'married', 'i', 'am', 'going', 'to', 'drive', 'myself']\n",
      "['it', 'has', 'a', 'leak', 'whether', 'shakespeare', 'wrote', 'this', 'poem', 'or']\n",
      "['not', 'will', 'probably', 'remain', 'a', 'mystery', 'you', 'are', 'safe', 'here']\n",
      "['i', 'prefer', 'to', 'travel', 'alone', 'classifies', 'we', 'appreciate', 'your', 'kind']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    print(list1[i*10:i*10+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vous', 'demander', 'une', 'dernière', 'faveur', 'Je', 'vis', 'dans', 'un', 'quartier']\n",
      "['relativement', 'sûr', 'Tom', 'et', 'Marie', 'étaient', 'sur', 'la', 'même', 'longueur']\n",
      "['d’onde', 'Kripananda', 'Cet', 'endroit', 'est', 'ennuyeux', 'Quel', 'est', 'votre', 'dentifrice']\n",
      "['préféré\\u202f', 'qualifié', 'postulé', 'complément', 'Pourquoi', 'ne', 'me', 'dites-vous', 'pas', 'ce']\n",
      "['que', 'vous', 'voulez', 'entendre', '', \"Quelqu'un\", 'nettoya', 'ma', 'chambre', 'pendant']\n",
      "['que', \"j'étais\", 'parti', 'Il', 'a', 'vécu', 'seul', 'dans', 'la', 'forêt']\n",
      "['Je', 'me', 'sens', 'mal', 'de', 'ne', 'pas', 'encore', \"t'avoir\", 'payé']\n",
      "['Êtes-vous', 'déjà', 'mariée', '', 'Je', 'vais', 'conduire', 'par', 'mes', 'propres']\n",
      "['moyens', 'Elle', 'fuit', 'Si', 'Shakespeare', 'a', 'écrit', 'ce', 'poème', 'ou']\n",
      "['pas', 'restera', 'probablement', 'une', 'énigme', 'Vous', 'êtes', 'en', 'sécurité', 'ici']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    print(list2[i*10:i*10+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üáîōœôëéçïùàûâèê\n"
     ]
    }
   ],
   "source": [
    "#some extra characters in french that are not in english\n",
    "my_extra_list = ['À', 'Â', 'Ç', 'É', 'Ê', 'Ô', 'à', 'á', 'â', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ï', 'ô', 'ù', 'û', 'ü', 'ō', 'œ']\n",
    "my_extra_list = [i.lower() for i in my_extra_list]\n",
    "my_extra_list = list(set(my_extra_list))\n",
    "\n",
    "my_extra_string = ''.join(my_extra_list)\n",
    "print(my_extra_string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct_2(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[^ a-zçèœêâôïûùüàáōëîé.?!,¿]\", \"\")\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "english_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length,\n",
    "    standardize=tf_lower_and_split_punct\n",
    ")\n",
    "\n",
    "french_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length+1,\n",
    "    standardize=tf_lower_and_split_punct_2\n",
    ")\n",
    "\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_fre_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "english_vectorization.adapt(train_eng_texts)\n",
    "french_vectorization.adapt(train_fre_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vectorization layers\n",
    "\n",
    "english_vocab = english_vectorization.get_vocabulary()\n",
    "    \n",
    "with open('text_vectorization_files/english_vocab_for_eng_fre.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(english_vocab, f)\n",
    "    \n",
    "french_vocab = french_vectorization.get_vocabulary()\n",
    "    \n",
    "with open('text_vectorization_files/french_vocab_for_eng_fre.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(french_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'i', 'you', 'to', 'the', 'is', '?', 'not', 'a', 'do', 'that', 'are', 'it', 'have', 'tom', 'he']\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'je', 'de', 'pas', '?', 'que', 'ne', 'à', 'la', 'le', 'vous', 'il', 'tom', 'est', ',', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(len(english_vocab))\n",
    "print(english_vocab[:20])\n",
    "print(french_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5606"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [x for x in english_vocab if len(x)==10]\n",
    "len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    eng = english_vectorization(eng)\n",
    "    fre = french_vectorization(spa)\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": fre[:, :-1],\n",
    "        },\n",
    "        fre[:, 1:],\n",
    "    )\n",
    "    \n",
    "def make_dataset(pairs):\n",
    "    eng_texts, fre_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    fre_texts = list(fre_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fre_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.cache().shuffle(2048).prefetch(16)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "test_ds = make_dataset(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2   35  277   12 3328 1189    4    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [   2   69   54  552 1218    7  342   25    5   75   34  123  121  165\n",
      "   187  159    4    3    0    0]], shape=(2, 20), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   2   34   19  314   26 4356 4333    4    3    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [   2   49   31   42  338  793   13  517   18    5  201   66    6  649\n",
      "    11   12  103  185  210    4]], shape=(2, 20), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  34   19  314   26 4356 4333    4    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  49   31   42  338  793   13  517   18    5  201   66    6  649   11\n",
      "    12  103  185  210    4    3]], shape=(2, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inp,tar in train_ds.take(1):\n",
    "    print(inp['encoder_inputs'][:2])\n",
    "    print(inp['decoder_inputs'][:2])\n",
    "    print(tar[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "units_1 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim = embed_dim\n",
    "        )\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(dense_dim,activation = 'relu'),\n",
    "                tf.keras.layers.Dense(embed_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self,inputs,mask = None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:,None,:],dtype = tf.int32)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        attention_output = self.mha(\n",
    "            query = inputs,\n",
    "            value = inputs,\n",
    "            key = inputs,\n",
    "            attention_mask = padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(attention_output+inputs)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        output = self.layernorm_2(proj_input + proj_output)\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim':self.embed_dim,\n",
    "            'dense_dim':self.dense_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self,sequence_length,vocab_size,embed_dim,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim = vocab_size,\n",
    "            output_dim  = embed_dim\n",
    "        )\n",
    "\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim = sequence_length,\n",
    "            output_dim = embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start = 0,limit = length,delta =1)\n",
    "        embeded_tokens = self.token_embeddings(inputs)\n",
    "        embeded_position = self.position_embeddings(positions)\n",
    "        return embeded_tokens + embeded_position\n",
    "    \n",
    "    def compute_mask(self,inputs,mask =None):\n",
    "        if mask is not None:\n",
    "            return tf.not_equal(inputs,0)\n",
    "        else:\n",
    "            return None\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'embed_dim': self.embed_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,embed_dim,latent_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads = num_heads,key_dim=embed_dim)\n",
    "\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(latent_dim,activation = 'relu'),\n",
    "                tf.keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self,inputs,encoder_outputs,mask=None):\n",
    "        casual_mask = self.get_casual_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:,None,:],dtype = tf.int32)\n",
    "            padding_mask = tf.minimum(padding_mask,casual_mask)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output1 = self.mha1(\n",
    "            query = inputs,\n",
    "            value = inputs,\n",
    "            key = inputs,\n",
    "            attention_mask = casual_mask\n",
    "        )\n",
    "\n",
    "        out_1 = self.layernorm_1(inputs + attention_output1)\n",
    "\n",
    "        attention_output2 = self.mha2(\n",
    "            query = out_1,\n",
    "            value = encoder_outputs,\n",
    "            key = encoder_outputs,\n",
    "            attention_mask = padding_mask,\n",
    "        )\n",
    "\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output2)\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        output = self.layernorm_3(proj_output + out_2)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_casual_attention_mask(self,inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size,sequence_length = input_shape[0],input_shape[1]\n",
    "        i = tf.range(sequence_length)[:,None]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i>=j,tf.int32)\n",
    "        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [\n",
    "                tf.expand_dims(batch_size,-1),\n",
    "                tf.convert_to_tensor([1,1]),\n",
    "            ],\n",
    "            axis =0,\n",
    "        )\n",
    "        return tf.tile(mask,mult)\n",
    "    \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'num_heads': self.num_heads\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define emmbedding dimensions, latent dimensions, and number of heads\n",
    "embed_dim = 100\n",
    "latent_dim = 256\n",
    "num_heads = 2\n",
    "\n",
    "#Encoder\n",
    "encoder_inputs = tf.keras.Input(shape = (None,), dtype = \"int64\", name = \"encoder_inputs\")\n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_inputs, encoder_outputs, name = \"encoder\")\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = tf.keras.Input(shape = (None,), dtype = \"int64\", name = \"decoder_inputs\")\n",
    "encoder_seq_inputs = tf.keras.Input(shape = (None, embed_dim), name = \"encoder_seq_inputs\")\n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoder_seq_inputs)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "decoder_outputs = tf.keras.layers.Dense(vocab_size, activation = tf.nn.log_softmax)(x)\n",
    "\n",
    "decoder = tf.keras.Model([decoder_inputs, encoder_seq_inputs], decoder_outputs, name = \"decoder\")\n",
    "\n",
    "# Define the final model\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = tf.keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name = \"transformer\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20, 15000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = transformer((inp['encoder_inputs'],inp['decoder_inputs']))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,502,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">132,656</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,230,556</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │ transformer_enco… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │  \u001b[38;5;34m1,502,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │    \u001b[38;5;34m132,656\u001b[0m │ positional_embed… │\n",
       "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m3,230,556\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m15000\u001b[0m)            │            │ transformer_enco… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,865,212</span> (18.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,865,212\u001b[0m (18.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,865,212</span> (18.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,865,212\u001b[0m (18.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1\n",
    "losses = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "transformer.summary()\n",
    "\n",
    "transformer.compile(\n",
    "    \"adam\", loss = losses, metrics = [\"accuracy\"]\n",
    ")\n",
    "# transformer.fit(train_ds, epochs = epochs, validation_data = val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 98 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "transformer.load_weights('model_weights/english_french_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>receptions</td>\n",
       "      <td>réceptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recessions</td>\n",
       "      <td>récessions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vindicated unoccupied</td>\n",
       "      <td>justifié inoccupé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diagonally experionce</td>\n",
       "      <td>en diagonale expérience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airrelated</td>\n",
       "      <td>lié à l'air</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 English                   French\n",
       "0             receptions               réceptions\n",
       "1             recessions               récessions\n",
       "2  vindicated unoccupied        justifié inoccupé\n",
       "3  diagonally experionce  en diagonale expérience\n",
       "4             airrelated              lié à l'air"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = pd.read_csv('data/english_french_10_letter.csv',index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('receptions', 'réceptions'),\n",
       " ('recessions', 'récessions'),\n",
       " ('vindicated unoccupied', 'justifié inoccupé'),\n",
       " ('diagonally experionce', 'en diagonale expérience'),\n",
       " ('airrelated', \"lié à l'air\")]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs_1= list(zip(my_df['English'],my_df['French']))\n",
    "test_pairs_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: autonomous\n",
      "correct_translation: autonome\n",
      "translated: [SOS] autonome [EOS]\n",
      "\n",
      "input: arunachala\n",
      "correct_translation: arunachala\n",
      "translated: [SOS] arunachala [EOS]\n",
      "\n",
      "input: authorship\n",
      "correct_translation: paternité\n",
      "translated: [SOS] paternité [EOS]\n",
      "\n",
      "input: aspiration\n",
      "correct_translation: aspiration\n",
      "translated: [SOS] aspiration [EOS]\n",
      "\n",
      "input: scavengers\n",
      "correct_translation: charognards\n",
      "translated: [SOS] charognards [EOS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "french_vocab = french_vectorization.get_vocabulary()\n",
    "french_index_lookup = dict(zip(range(len(french_vocab)), french_vocab))\n",
    "max_decoded_sentence_length = sequence_length\n",
    "\n",
    "def decode_sentence(input_sentence):\n",
    "    tokenized_input_sentence = english_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[SOS]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = french_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = tf.argmax(predictions[0, i, :]).numpy().item(0)\n",
    "        sampled_token = french_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[EOS]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "my_list1 = []\n",
    "for _ in range(5):\n",
    "    input_sentence,output_sentence = random.choice(test_pairs_1)\n",
    "    my_list1.append(input_sentence)\n",
    "    input_sentence = input_sentence.lower()\n",
    "    translated = decode_sentence(input_sentence)\n",
    "    print(f\"input: {input_sentence}\")\n",
    "    print(f'correct_translation: {output_sentence}')\n",
    "    print(f\"translated: {translated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some elembents\n",
    "my_list = ['collection deflection remissions articulate terrrorist catwalking celebrated',\n",
    "           'bharatendu mujahideen mobilizing interrupts transpires classmates jahawarlal',\n",
    "            'evangelist habituated ascendancy currencies enactments contracted', \n",
    "            'switchover vaisheshik assistance aparbrahma undetected applicants', \n",
    "            'mistresses suchindram recuperate prosecuted deforested glorifying', \n",
    "            'inoculated pleasantly thereafter revengeful humiliated vibheeshan parushuram kurukhatra',\n",
    "            'portuguese electicity shrotyagya',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'collection deflection remissions articulate terrrorist catwalking celebrated bharatendu mujahideen mobilizing interrupts transpires classmates jahawarlal evangelist habituated ascendancy currencies enactments contracted switchover vaisheshik assistance aparbrahma undetected applicants mistresses suchindram recuperate prosecuted deforested glorifying inoculated pleasantly thereafter revengeful humiliated vibheeshan parushuram kurukhatra dharmshala pakisthaan swapurusha portuguese electicity shrotyagya'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
