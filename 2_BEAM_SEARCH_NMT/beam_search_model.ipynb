{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading weights to confirm translation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\eng_spa.csv',names= ['English','Spanish'],index_col =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>[start] Ve. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>[start] Vete. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>[start] Vaya. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>[start] Váyase. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>[start] Hola. [end]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English                Spanish\n",
       "0     Go.      [start] Ve. [end]\n",
       "1     Go.    [start] Vete. [end]\n",
       "2     Go.    [start] Vaya. [end]\n",
       "3     Go.  [start] Váyase. [end]\n",
       "4     Hi.    [start] Hola. [end]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118964"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                      Ve. \n",
       "1                                                    Vete. \n",
       "2                                                    Vaya. \n",
       "3                                                  Váyase. \n",
       "4                                                    Hola. \n",
       "                                ...                        \n",
       "118959     Hay cuatro causas principales de muertes rela...\n",
       "118960     Hay madres y padres que se quedan despiertos ...\n",
       "118961     Una huella de carbono es la cantidad de conta...\n",
       "118962     Como suele haber varias páginas web sobre cua...\n",
       "118963     Si quieres sonar como un hablante nativo, deb...\n",
       "Name: Spanish, Length: 118964, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Spanish'] = df['Spanish'].apply(lambda x:x.lstrip('[start]'))\n",
    "df['Spanish'] = df['Spanish'].apply(lambda x:x.rstrip('[end]'))\n",
    "df['Spanish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How long have you been studying Hungarian?',\n",
       "  ' ¿Cuánto tiempo has estado estudiando húngaro? '),\n",
       " ('Do you really want to be here?', ' ¿Realmente querés estar acá? '),\n",
       " ('She is as beautiful as Snow White.', ' Ella es bella como Blancanieves. '),\n",
       " (\"There are few men who don't know that.\",\n",
       "  ' Hay pocos hombres que no lo saben. '),\n",
       " ('Tom changes channels during commercials.',\n",
       "  ' Tom cambia de canal durante los comerciales. ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(zip(df['English'],df['Spanish']))\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "95172 training pairs\n",
      "23792 validation pairs\n"
     ]
    }
   ],
   "source": [
    "len_val = int(0.2 * len(data))\n",
    "len_train = len(data) - len_val\n",
    "train_pairs = data[:len_train]\n",
    "val_pairs = data[len_train:len_train + len_val]\n",
    "\n",
    "print(f\"{len(data)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 12000\n",
    "sequence_length = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text\n",
    "\n",
    "english_vectorization =tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    output_mode = 'int',\n",
    "    ragged=True,\n",
    "    max_tokens=vocab_size,\n",
    "    # output_sequence_length = 20\n",
    ")\n",
    "\n",
    "spanish_vectorization =tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    output_mode = 'int',\n",
    "    ragged = True,\n",
    "    max_tokens=vocab_size,\n",
    "    # output_sequence_length=21\n",
    ")\n",
    "english_data = [x[0] for x in train_pairs]\n",
    "spanish_data = [x[1] for x in train_pairs]\n",
    "english_vectorization.adapt(english_data)\n",
    "spanish_vectorization.adapt(spanish_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vectorization layers\n",
    "english_vocab = english_vectorization.get_vocabulary()\n",
    "    \n",
    "with open('text_vectorization_files/english_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(english_vocab, f)\n",
    "    \n",
    "spanish_vocab = spanish_vectorization.get_vocabulary()\n",
    "    \n",
    "with open('text_vectorization_files/spanish_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(spanish_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary = spanish_vocab,\n",
    "    mask_token = \"\",\n",
    "    oov_token = '[UNK]'\n",
    ")\n",
    "\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary = spanish_vocab,\n",
    "    mask_token = '',\n",
    "    oov_token = '[UNK]',\n",
    "    invert = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(tokens, id_to_word):\n",
    "    words = id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=\" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'the', 'i', 'to', 'you', 'tom']\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'de', 'que', 'a', 'no', 'tom']\n"
     ]
    }
   ],
   "source": [
    "print(english_vocab[:10])\n",
    "print(spanish_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = word_to_id('[SOS]')\n",
    "eos_id = word_to_id('[EOS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(data, english_vectorization,spanish_vectorization):\n",
    "    eng_data = [x[0] for x in data]\n",
    "    \n",
    "    spa_data = [x[1] for x in data]\n",
    " \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_data,spa_data)).batch(batch_size = batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(train_pairs,english_vectorization,spanish_vectorization)\n",
    "val_dataset = generate_dataset(val_pairs,english_vectorization,spanish_vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(context,target):\n",
    "    context = english_vectorization(context)\n",
    "    context = context.to_tensor()\n",
    "    target = spanish_vectorization(target)\n",
    "    targ_in = target[:,:-1].to_tensor()\n",
    "    targ_out = target[:,1:].to_tensor()\n",
    "    return (context,targ_in),targ_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(preprocess_text,tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(preprocess_text,tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2   55  148   21    8   86  488 5127   11    3    0    0    0    0\n",
      "     0    0    0    0    0]\n",
      " [   2   20    8  121   37    7   35   63   11    3    0    0    0    0\n",
      "     0    0    0    0    0]], shape=(2, 19), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[    2    12   205    61   124   157   729 11280    11     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    2    12   204   482    96   602    11     0     0     0     0     0\n",
      "      0     0     0     0     0]], shape=(2, 17), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   12   205    61   124   157   729 11280    11     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   12   204   482    96   602    11     3     0     0     0     0     0\n",
      "      0     0     0     0     0]], shape=(2, 17), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   2    9   40   32   28  334   61   29 1288   20   34 3682  111    4\n",
      "     3    0    0    0    0    0    0]\n",
      " [   2   17   18   59   25  191    4    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]], shape=(2, 21), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   2    9   36   34  102  381    5   29  590    7   55   13   89    5\n",
      "  2168 5142    4    0    0    0]\n",
      " [   2   60   51   33   72    4    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]], shape=(2, 20), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   9   36   34  102  381    5   29  590    7   55   13   89    5 2168\n",
      "  5142    4    3    0    0    0]\n",
      " [  60   51   33   72    4    3    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]], shape=(2, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (x,y),z in train_dataset.take(2):\n",
    "    print(x[:2])\n",
    "    print(y[:2])\n",
    "    print(z[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_1 = 12000\n",
    "units_1 = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size = vocab_size_1,units = units_1):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.units =units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = units,input_shape = (None,),mask_zero=True)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(merge_mode='sum',layer = tf.keras.layers.LSTM(units,return_sequences= True))\n",
    "\n",
    "    def call(self,encoder_inputs):\n",
    "\n",
    "        embedded_output = self.embedding(encoder_inputs)\n",
    "        output = self.lstm(embedded_output)\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"units\": self.units\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size,units_1)\n",
    "\n",
    "output_1 = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 21, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units=units_1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.units =units\n",
    "\n",
    "        self.mha = (tf.keras.layers.MultiHeadAttention(key_dim= units,num_heads=1))\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self,context,target):\n",
    "\n",
    "        attn_output = self.mha(query = target,value = context)\n",
    "        x = self.add([target,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"units\": self.units\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'cross_attention' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention =CrossAttention(units_1)\n",
    "# input = tf.keras.layers.Input(shape =(None,))\n",
    "target = tf.keras.layers.Embedding(vocab_size_1,units_1,input_shape = (None,None),mask_zero=True)(y)\n",
    "# input = tf.keras.layers.Input(shape =(None,units_1))\n",
    "output_2 = attention(output_1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20, 128])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size = vocab_size_1,units = units_1):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = units,mask_zero=True)\n",
    "        self.pre_attention_rnn = tf.keras.layers.LSTM(units,return_sequences = True,return_state = True)\n",
    "        self.attention = CrossAttention(units)\n",
    "        self.post_attention_rnn = tf.keras.layers.LSTM(units = units,return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size,activation = tf.nn.log_softmax)\n",
    "\n",
    "    def call(self,context,target,state = None,return_state = False):\n",
    "\n",
    "        embedding_output = self.embedding(target)\n",
    "        x,state_h,state_c = self.pre_attention_rnn(embedding_output,initial_state=state)\n",
    "        x = self.attention(context,x)\n",
    "        x = self.post_attention_rnn(x)\n",
    "        logits = self.dense(x)\n",
    "\n",
    "        if return_state:\n",
    "            return logits,[state_h,state_c]\n",
    "\n",
    "        return logits\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"units\": self.units\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'cross_attention_1' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'decoder' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size,units_1)\n",
    "output= decoder(output_1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20, 12000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self,vocab_size =vocab_size_1,units = units_1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size,units)\n",
    "        self.decoder = Decoder(vocab_size,units)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        context,target = inputs\n",
    "        encoder_output = self.encoder(context)\n",
    "        logits = self.decoder(encoder_output,target)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'cross_attention_2' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'decoder_1' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20, 12000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator(vocab_size_1,units_1)\n",
    "outputs = translator((x,y))\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"translator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"translator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,413,472</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m1,799,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m3,413,472\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,212,640</span> (19.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,212,640\u001b[0m (19.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,212,640</span> (19.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,212,640\u001b[0m (19.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translator.compile(optimizer = 'adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics = ['accuracy'])\n",
    "translator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "translator.load_weights(\"model_weights/english_to_spanish.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model,text,max_length = 30,beam_width= 1,english_vectorizer = english_vectorization):\n",
    "\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "\n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "\n",
    "    context = model.encoder(context)\n",
    "    state = [tf.zeros((1,units_1)),tf.zeros((1,units_1))]\n",
    "\n",
    "    end_token = tf.fill((1,1),eos_id)\n",
    "\n",
    "    done  = False\n",
    "\n",
    "    sequences = [[[sos_id.numpy()],0.0,state]]\n",
    "\n",
    "    final_sequences = []\n",
    "    k = beam_width\n",
    "    for i in range(max_length):\n",
    "\n",
    "        if len(final_sequences)<k:\n",
    "\n",
    "            if len(sequences)>k:\n",
    "                sequences.sort(key = lambda x: x[1],reverse = True)\n",
    "                sequences = sequences[:k]\n",
    "            \n",
    "            pre_sequences = []\n",
    "            for i in range(len(sequences)):\n",
    "\n",
    "                cur_sequence = sequences[i]\n",
    "\n",
    "                cur_token = tf.cast(tf.fill((1,1),cur_sequence[0][-1]),end_token.dtype)\n",
    "\n",
    "                cur_state = cur_sequence[2]\n",
    "\n",
    "                if cur_token == end_token:\n",
    "                    final_sequences.append(copy.deepcopy(cur_sequence))\n",
    "                    continue\n",
    "\n",
    "                next_tokens,state,next_logits = generate_next_token(decoder = model.decoder,\n",
    "                                                                        context = context,\n",
    "                                                                        next_token = cur_token,\n",
    "                                                                        state = cur_state,\n",
    "                                                                        beam_width = k)\n",
    "                \n",
    "    \n",
    "                my_sequences = [copy.deepcopy(cur_sequence) for x in range(k)]\n",
    "\n",
    "                for i in range(len(my_sequences)):\n",
    "                    my_sequences[i][0].append(next_tokens[i])\n",
    "\n",
    "                    my_sequences[i][1]+=next_logits[i]\n",
    "\n",
    "                    my_sequences[i][2] = state\n",
    "\n",
    "                pre_sequences+=my_sequences\n",
    "                \n",
    "\n",
    "            sequences = pre_sequences\n",
    "\n",
    "    def cleaning(list_sequences):\n",
    "        my_list = []\n",
    "        list_sequences.sort(key = lambda x: x[1],reverse = True)\n",
    "        \n",
    "        if len(list_sequences)>k:\n",
    "                list_sequences = list_sequences[:k]\n",
    "        for sequence in list_sequences:\n",
    "            my_tokens = sequence[0]\n",
    "            score = sequence[1]\n",
    "            translation = tokens_to_text(my_tokens,id_to_word)\n",
    "            translation = translation.numpy().decode()\n",
    "            my_list.append([translation,f'score: {round(score,3)}'])\n",
    "        return my_list\n",
    "    return cleaning(final_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(decoder,context,next_token,state,beam_width):\n",
    "    \n",
    "    logits,state = decoder(context,next_token,state,return_state = True)\n",
    "\n",
    "    logits = logits[:,-1,:]\n",
    "\n",
    "    next_logits , next_tokens = tf.nn.top_k(logits,k = beam_width)\n",
    "\n",
    "    next_logits = tf.squeeze(next_logits).numpy()\n",
    "\n",
    "    next_tokens = tf.squeeze(next_tokens).numpy()\n",
    "    if beam_width == 1:\n",
    "        return [next_tokens],state,[next_logits]\n",
    "\n",
    "    return next_tokens,state,next_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data = [x[0] for x in data]\n",
    "spa_data = [x[1] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It didn't take Tom long to realize he wasn't welcome there.\n",
      " No le tomó mucho a Tom darse cuenta de que no era bienvenido allí. \n"
     ]
    }
   ],
   "source": [
    "p = 12345\n",
    "print(eng_data[p])\n",
    "print(spa_data[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the loss here is not probability but log probability because we used activation as tf.nn.log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[SOS] no escrib a tom mucho suficiente cuenta que no era bienvenido all . [EOS]',\n",
       "  'score: -13.274'],\n",
       " ['[SOS] no escrib a tom mucho suficiente cuenta que no fue bienvenido all . [EOS]',\n",
       "  'score: -14.214'],\n",
       " ['[SOS] no escrib a tom mucho suficiente cuenta que no era bienvenido ah . [EOS]',\n",
       "  'score: -14.807']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(translator,eng_data[p],beam_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
