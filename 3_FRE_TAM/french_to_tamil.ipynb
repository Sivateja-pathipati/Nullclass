{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of df:  125548\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/french_tamil_0.csv',index_col = 'Unnamed: 0')\n",
    "\n",
    "print('length of df: ',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx length of french vocabulary:  31609\n",
      "approx size of five_letter_words in fre_vocabulary 2977\n",
      "approx length of Tamil vocabulary:  31609\n"
     ]
    }
   ],
   "source": [
    "my_list1 = list(df['French'])\n",
    "my_list1 = [x for y in my_list1 for x in y.split(' ')]\n",
    "my_list1 = list(set(my_list1))\n",
    "\n",
    "my_list2 = list(df['Tamil'])\n",
    "my_list2 = [x for y in my_list2 for x in y.split(' ')]\n",
    "my_list2 = list(set(my_list2))\n",
    "\n",
    "print('approx length of french vocabulary: ',len(my_list1))\n",
    "print('approx size of five_letter_words in fre_vocabulary',len([x for x in my_list1 if len(x)==5]))\n",
    "print('approx length of Tamil vocabulary: ',len(my_list1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French</th>\n",
       "      <th>Tamil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tout le monde n’est pas comme toi.</td>\n",
       "      <td>எல்லோரும் உங்களைப் போல் இல்லை.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L'avion volait au-dessus des nuages.</td>\n",
       "      <td>விமானம் மேகங்களுக்கு மேல் பறந்து கொண்டிருந்தது.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reste loin de moi.</td>\n",
       "      <td>என்னை விட்டு விலகி இரு.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Où as-tu eu ça ?</td>\n",
       "      <td>இது எங்கிருந்து கிடைத்தது?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Je n'avais pas le choix.</td>\n",
       "      <td>எனக்கு வேறு வழியில்லை.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 French  \\\n",
       "0    Tout le monde n’est pas comme toi.   \n",
       "1  L'avion volait au-dessus des nuages.   \n",
       "2                    Reste loin de moi.   \n",
       "3                      Où as-tu eu ça ?   \n",
       "4              Je n'avais pas le choix.   \n",
       "\n",
       "                                             Tamil  \n",
       "0                   எல்லோரும் உங்களைப் போல் இல்லை.  \n",
       "1  விமானம் மேகங்களுக்கு மேல் பறந்து கொண்டிருந்தது.  \n",
       "2                          என்னை விட்டு விலகி இரு.  \n",
       "3                       இது எங்கிருந்து கிடைத்தது?  \n",
       "4                           எனக்கு வேறு வழியில்லை.  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows that contain english alphabets in tamil:  388\n",
      "rows that contain english alphabets in tamil after elimination:  0\n"
     ]
    }
   ],
   "source": [
    "my_list = list(df['Tamil'].apply(lambda x: re.findall('[a-zA-Z]+',x)))\n",
    "print('rows that contain english alphabets in tamil: ',len([x for x in my_list if len(x)]))\n",
    "indexes = []\n",
    "for i,j in enumerate(my_list):\n",
    "    if len(j):\n",
    "        # print(j)\n",
    "        indexes.append(i)\n",
    "df.drop(df.index[indexes],inplace =True,axis = 0)\n",
    "\n",
    "#cross checking once again\n",
    "my_list = list(df['Tamil'].apply(lambda x: re.findall('[a-zA-Z]+',x)))\n",
    "print('rows that contain english alphabets in tamil after elimination: ',len([x for x in my_list if len(x)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tout le monde n’est pas comme toi.', \"L'avion volait au-dessus des nuages.\", 'Reste loin de moi.', 'Où as-tu eu ça ?', \"Je n'avais pas le choix.\"]\n",
      "['எல்லோரும் உங்களைப் போல் இல்லை.', 'விமானம் மேகங்களுக்கு மேல் பறந்து கொண்டிருந்தது.', 'என்னை விட்டு விலகி இரு.', 'இது எங்கிருந்து கிடைத்தது?', 'எனக்கு வேறு வழியில்லை.']\n"
     ]
    }
   ],
   "source": [
    "french_list = df['French'].tolist()\n",
    "tamil_list = df['Tamil'].tolist()\n",
    "print(french_list[:5])\n",
    "print(tamil_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Appelez la police!', 'காவல்துறையை அழைக்கவும்!'), ('Être conscient de ce que nous mangeons et de quelle quantité est essentiel à une bonne santé.', 'நாம் என்ன சாப்பிடுகிறோம், எவ்வளவு சாப்பிடுகிறோம் என்பதை அறிந்திருப்பது நல்ல ஆரோக்கியத்திற்கு அவசியம்.'), ('Elle a enseigné la musique pendant trente ans.', 'முப்பது வருடங்கள் இசை கற்பித்தார்.'), ('Tom a perdu contact avec Mary.', 'டாம் மேரி உடனான தொடர்பை இழந்தார்.'), ('Tom veut que son père soit enterré à côté de sa mère.', 'டாம் தனது தந்தையை தனது தாயின் அருகில் அடக்கம் செய்ய விரும்புகிறார்.')]\n"
     ]
    }
   ],
   "source": [
    "text_pairs =list(zip(french_list,tamil_list))\n",
    "random.seed(42)\n",
    "random.shuffle(text_pairs)\n",
    "text_pairs = text_pairs[:100000]\n",
    "print(text_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 total pairs\n",
      "80000 training pairs\n",
      "10000 validation pairs\n",
      "10000 test pairs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_val_samples = int(0.1 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) -  2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f'{len(test_pairs)} test pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_1 = 10000\n",
    "vocab_size_2 = 12000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct_1(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "fre_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = vocab_size_1,\n",
    "    output_mode = \"int\",\n",
    "    ragged = True,\n",
    "    standardize=tf_lower_and_split_punct\n",
    ")\n",
    "\n",
    "tam_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = vocab_size_2,\n",
    "    output_mode = \"int\",\n",
    "    ragged = True,\n",
    "    standardize=tf_lower_and_split_punct_1\n",
    ")\n",
    "\n",
    "train_fre_texts = [pair[0] for pair in train_pairs]\n",
    "train_tam_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "fre_vectorization.adapt(train_fre_texts)\n",
    "tam_vectorization.adapt(train_tam_texts)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vectorization layers\n",
    "\n",
    "fre_vocab = fre_vectorization.get_vocabulary()    \n",
    "with open('text_vectorization_files/fre_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(fre_vocab, f)\n",
    "\n",
    "tam_vocab = tam_vectorization.get_vocabulary()\n",
    "with open('text_vectorization_files/tam_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tam_vocab, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "12000\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'de', 'je', 'tom', 'pas', 'la', '?', 'le', 'a', 'que', 'ne', 'il', 'est', 'vous', 'un', ',']\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'நான்', 'டாம்', '?', 'நீங்கள்', 'ஒரு', 'என்று', 'வேண்டும்', ',', 'அவர்', 'எனக்கு', 'இந்த', 'உங்கள்', 'என்', 'மிகவும்', 'அவள்']\n",
      "size of five letter words in french_vocab:  1655\n"
     ]
    }
   ],
   "source": [
    "print(len(fre_vocab))\n",
    "print(len(tam_vocab))\n",
    "print(fre_vocab[:20])\n",
    "print(tam_vocab[:20])\n",
    "five_letter_words = [x for x in fre_vocab if len(x)==5]\n",
    "print('size of five letter words in french_vocab: ',len(five_letter_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer_size = 10000\n",
    "batch_size = 64\n",
    "def generate_dataset(pairs):\n",
    "    fre_data = [x[0] for x in pairs]\n",
    "    tam_data = [x[1] for x in pairs]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((fre_data,tam_data)).shuffle(Buffer_size).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    }
   ],
   "source": [
    "train_dataset = generate_dataset(train_pairs)\n",
    "val_dataset = generate_dataset(val_pairs)\n",
    "test_dataset = generate_dataset(test_pairs)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(fre_data,tam_data):\n",
    "    fre_ids = fre_vectorization(fre_data).to_tensor()\n",
    "    tam_ids = tam_vectorization(tam_data)\n",
    "    target_in = tam_ids[:,:-1].to_tensor()\n",
    "    target_out = tam_ids[:,1:].to_tensor()\n",
    "    return (fre_ids,target_in),target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n",
      "157\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(process,tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(process,tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(process,tf.data.AUTOTUNE)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2   21  309    8 9224    4    3    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   2  267  993    3    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(2, 26), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   2   20 1682 7146    4    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]\n",
      " [   2  348   85    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]], shape=(2, 19), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  20 1682 7146    4    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]\n",
      " [ 348   85    3    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]], shape=(2, 19), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (x,y),z in train_dataset.take(1):\n",
    "    print(x[:2])\n",
    "    print(y[:2])\n",
    "    print(z[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary = tam_vocab,\n",
    "    mask_token = \"\",\n",
    "    oov_token = '[UNK]'\n",
    ")\n",
    "\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary = tam_vocab,\n",
    "    mask_token = '',\n",
    "    oov_token = '[UNK]',\n",
    "    invert = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_string(ints):\n",
    "  strs = [chr(i) for i in ints]\n",
    "  joined = ''.join(strs)\n",
    "  return joined\n",
    "\n",
    "def tokens_to_text(tokens, id_to_word):\n",
    "    words = id_to_word(tokens)\n",
    "\n",
    "    try:\n",
    "       result = tf.strings.reduce_join(words, axis=-1, separator=\" \").numpy()\n",
    "    except:\n",
    "      result = words.numpy()\n",
    "\n",
    "    decoded = tf.strings.unicode_decode(result,'utf-8').numpy()\n",
    "    decoded_sentence = decode_string(decoded)\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sos_id = word_to_id('[SOS]')\n",
    "eos_id = word_to_id('[EOS]')\n",
    "print(sos_id)\n",
    "print(eos_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_1 = 10000\n",
    "vocab_size_2 = 12000\n",
    "units_1 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,fre_vocab_size = vocab_size_1,units = units_1):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = fre_vocab_size\n",
    "        self.units =units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = fre_vocab_size,output_dim = units,mask_zero=True)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(merge_mode='sum',layer = tf.keras.layers.LSTM(units,return_sequences= True))\n",
    "\n",
    "    def call(self,encoder_inputs):\n",
    "\n",
    "        embedded_output = self.embedding(encoder_inputs)\n",
    "        output = self.lstm(embedded_output)\n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 128])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size_1,units_1)\n",
    "output1 = encoder(x)\n",
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units=units_1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.units =units\n",
    "\n",
    "        self.mha = (tf.keras.layers.MultiHeadAttention(key_dim= units,num_heads=1))\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.support_masking = True\n",
    "    def call(self,context,target):\n",
    "\n",
    "        attn_output = self.mha(query = target,value = context)\n",
    "        x = self.add([target,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'cross_attention_3' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 19, 128])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = CrossAttention(units_1)\n",
    "embed_output = tf.keras.layers.Embedding(vocab_size_2,units_1)(y)\n",
    "output2  = attention(output1,embed_output)\n",
    "output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,fre_vocab_size = vocab_size_1,units = units_1,tam_vocab_size = vocab_size_2):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.fre_vocab_size = fre_vocab_size\n",
    "        self.tam_vocab_size = tam_vocab_size\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = tam_vocab_size,output_dim = units,mask_zero=True)\n",
    "        self.pre_attention_rnn = tf.keras.layers.LSTM(units,return_sequences = True,return_state = True)\n",
    "        self.attention = CrossAttention(units)\n",
    "        self.post_attention_rnn = tf.keras.layers.LSTM(units = units,return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(tam_vocab_size,activation = tf.nn.log_softmax)\n",
    "        self.support_masking = True\n",
    "    def call(self,context,target,state = None,return_state = False):\n",
    "\n",
    "        embedding_output = self.embedding(target)\n",
    "        x,state_h,state_c = self.pre_attention_rnn(embedding_output,initial_state=state)\n",
    "        x = self.attention(context,x)\n",
    "        x = self.post_attention_rnn(x)\n",
    "        logits = self.dense(x)\n",
    "\n",
    "        if return_state:\n",
    "            return logits,[state_h,state_c]\n",
    "\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'cross_attention_4' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'decoder_2' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 19, 12000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size_1,units_1)\n",
    "output3 = decoder(output1,y)\n",
    "output3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self,fre_vocab_size =vocab_size_1,units = units_1,tam_vocab_size = vocab_size_2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(fre_vocab_size,units)\n",
    "        self.decoder = Decoder(fre_vocab_size,units,tam_vocab_size)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        context,target = inputs\n",
    "        encoder_output = self.encoder(context)\n",
    "        logits = self.decoder(encoder_output,target)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'cross_attention_5' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'decoder_3' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 19, 12000])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator(vocab_size_1,units_1,vocab_size_2)\n",
    "logits = translator((x,y))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 19])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model,epochs =10,steps_per_epoch = 500):\n",
    "    model.compile(optimizer = 'adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction = 'none'),\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history  = model.fit(\n",
    "        train_dataset.repeat(),\n",
    "        epochs = epochs,\n",
    "        steps_per_epoch = steps_per_epoch,\n",
    "        validation_data = val_dataset,\n",
    "        validation_steps = 50,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.compile(optimizer = 'adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction = 'none'),\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 3.6363e-05 - loss: 4.4246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.444233417510986, 4.0404043829767033e-05]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "translator.load_weights('model_weights/french_to_tamil.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 76ms/step - accuracy: 0.3290 - loss: 0.8859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9046087861061096, 0.3260391652584076]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_0(decoder,context,next_token,done,state):\n",
    "    \n",
    "    logits,state = decoder(context,next_token,state,return_state = True)\n",
    "\n",
    "    logits = logits[:,-1,:]\n",
    "\n",
    "    next_token = tf.argmax(logits,axis = -1)\n",
    "\n",
    "    logits = tf.squeeze(logits)\n",
    "\n",
    "    next_token = tf.squeeze(next_token)\n",
    "\n",
    "    logit = logits[next_token].numpy()\n",
    "    \n",
    "    next_token = tf.reshape(next_token,shape  = (1,1))\n",
    "    \n",
    "    if next_token == eos_id :\n",
    "        done = True\n",
    "    return next_token,state,done,logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_0(model,text,max_length = 30,):\n",
    "    tokens,logits = [],[]\n",
    "    #condition to convert only five letter words\n",
    "    text = text.split(' ')\n",
    "    text = [x.strip().strip(',').strip('?').strip('!').strip('\"').strip('.') for x in text]\n",
    "    text = [x for x in text if len(x)==5]\n",
    "    # print('five _letter_words in input_text: ' ,len(text))\n",
    "    if len(text) == 0:\n",
    "        return 'The Text has no five letter words! Please try again'\n",
    "    text = ' '.join(text)\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    context = fre_vectorization(text).to_tensor()\n",
    "    context = model.encoder(context)\n",
    "    state = [tf.zeros((1,units_1)),tf.zeros((1,units_1))]\n",
    "    next_token = tf.fill((1,1),sos_id)\n",
    "    done  = False\n",
    "\n",
    "    for i in range(max_length):\n",
    "        try:\n",
    "            next_token,state,done,logit = generate_next_token_0(decoder = model.decoder,\n",
    "                                                                context = context,\n",
    "                                                                next_token = next_token,\n",
    "                                                                done = done,\n",
    "                                                                state = state,\n",
    "                                                              )\n",
    "        except:\n",
    "            raise Exception('generate next token code issue')\n",
    "        if done:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "        logits.append(logit)\n",
    "    tokens = tf.concat(tokens,axis = -1)\n",
    "    tokens = tf.squeeze(tokens)\n",
    "    \n",
    "    translation =tokens_to_text(tokens,id_to_word)\n",
    "\n",
    "    return translation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [\"S'il vous plaît, allez-y.\",\n",
    "           \"êtres\",\n",
    "           \"Je pense que je deviens fou.\",\n",
    "           \"Vous devriez arrêter de fumer car c'est malsain.\",\n",
    "           \"autre bûche\",\n",
    "           \" Je jouais de la flûte quand j'étais au lycée\",\n",
    "           ' cible quels huilé',\n",
    "           \"belle fille noirs était\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'cross_attention_2' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sivat\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'decoder_1' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('தயவுசெய்து',)\n",
      "('உயிரினங்கள்',)\n",
      "('நினைக்கிறார்கள்',)\n",
      "('இதை புகை',)\n",
      "('மற்றொரு பதிவு',)\n",
      "('உயர்நிலைப் பள்ளி [UNK] வந்த உடைந்த இடம்',)\n",
      "('எண்ணெய் தடவிய இலக்கு யாரென்று தொடர்',)\n",
      "('அழகான கருப்பு பெண்',)\n"
     ]
    }
   ],
   "source": [
    "for i in my_list[:]:\n",
    "    translation = translate_0(translator,i,)\n",
    "    print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "Surtout, soyez patient.\n",
      "எல்லாவற்றிற்கும் மேலாக, பொறுமையாக இருங்கள்.\n"
     ]
    }
   ],
   "source": [
    "fre_data = [x[0] for x in train_pairs]\n",
    "tam_data = [x[1] for x in train_pairs]\n",
    "print(len(fre_data))\n",
    "p = 67890\n",
    "print(fre_data[p])\n",
    "print(tam_data[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('இருக்கும்',)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_0(translator,fre_data[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_shape = 10 * 26 \n",
    "output = [[0.1, 0.01, 0.03, ... ... ... ... ... (len-26)],\n",
    "          [0.1, 0.002, 0.6, ... ... ... ... ... (len-26)],\n",
    "            ''         ''         ''      ''       ''    \n",
    "            ''         ''         ''      ''       '' \n",
    "          [0.9, 0.01, 0.01, ... ... ... ... ... (len-26)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "def greedy_search_decoder(predictions):\n",
    "  \n",
    "    #select token with the maximum probability for each prediction\n",
    "    output_sequence = [np.argmax(prediction) for prediction in predictions]\n",
    "    \n",
    "    #storing token probabilities\n",
    "    token_probabilities = [np.max(prediction) for prediction in predictions]\n",
    "    \n",
    "    #multiply individaul token-level probabilities to get overall sequence probability\n",
    "    sequence_probability = np.product(token_probabilities)\n",
    "    \n",
    "    return output_sequence, sequence_probability\n",
    "    \n",
    "model_prediction = [[0.1, 0.7, 0.1, 0.1],\n",
    "                    [0.7, 0.1, 0.1, 0.1],\n",
    "                    [0.1, 0.1, 0.6, 0.2],\n",
    "                    [0.1, 0.1, 0.1, 0.7],\n",
    "                    [0.4, 0.3, 0.2, 0.1]]\n",
    "                    \n",
    "greedy_search_decoder(model_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def beam_search_decoder(predictions, top_k = 3):\n",
    "    #start with an empty sequence with zero score\n",
    "    output_sequences = [([], 0)]\n",
    "    \n",
    "    #looping through all the predictions\n",
    "    for token_probs in predictions:\n",
    "        new_sequences = []\n",
    "        \n",
    "        #append new tokens to old sequences and re-score\n",
    "        for old_seq, old_score in output_sequences:\n",
    "            for char_index in range(len(token_probs)):\n",
    "                new_seq = old_seq + [char_index]\n",
    "                #considering log-likelihood for scoring\n",
    "                new_score = old_score + math.log(token_probs[char_index])\n",
    "                new_sequences.append((new_seq, new_score))\n",
    "                \n",
    "        #sort all new sequences in the de-creasing order of their score\n",
    "        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n",
    "        \n",
    "        #select top-k based on score \n",
    "        # *Note- best sequence is with the highest score\n",
    "        output_sequences = output_sequences[:top_k]\n",
    "        \n",
    "    return output_sequences\n",
    "    \n",
    "\n",
    "model_prediction = [[0.1, 0.7, 0.1, 0.1],\n",
    "                    [0.7, 0.1, 0.1, 0.1],\n",
    "                    [0.1, 0.1, 0.6, 0.2],\n",
    "                    [0.1, 0.1, 0.1, 0.7],\n",
    "                    [0.4, 0.3, 0.2, 0.1]]\n",
    "                    \n",
    "beam_search_decoder(model_prediction, top_k = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Out] : [([1, 0, 2, 3, 0], -2.497141187456343),\n",
    "         ([1, 0, 2, 3, 1], -2.784823259908124),\n",
    "         ([1, 0, 2, 3, 2], -3.1902883680162883),\n",
    "         ([1, 0, 3, 3, 0], -3.595753476124453),\n",
    "         ([1, 0, 2, 3, 3], -3.8834355485762337)]\n",
    "\n",
    "\n",
    "[Out]: ([1, 0, 2, 3, 0], 0.08231999999999998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "data = array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def greedy_decoder(data):\n",
    " # index for largest probability each row\n",
    " return [argmax(s) for s in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from numpy import argmax\n",
    " \n",
    "# greedy decoder\n",
    "def greedy_decoder(data):\n",
    " # index for largest probability each row\n",
    " return [argmax(s) for s in data]\n",
    " \n",
    "# define a sequence of 10 words over a vocab of 5 words\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "data = array(data)\n",
    "# decode sequence\n",
    "result = greedy_decoder(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4, 0, 4, 0, 4, 0, 4, 0, 4, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beam search\n",
    "def beam_search_decoder(data, k):\n",
    " sequences = [[list(), 0.0]]\n",
    " # walk over each step in sequence\n",
    " for row in data:\n",
    " all_candidates = list()\n",
    " # expand each current candidate\n",
    " for i in range(len(sequences)):\n",
    " seq, score = sequences[i]\n",
    " for j in range(len(row)):\n",
    " candidate = [seq + [j], score - log(row[j])]\n",
    " all_candidates.append(candidate)\n",
    " # order all candidates by score\n",
    " ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    " # select k best\n",
    " sequences = ordered[:k]\n",
    " return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    " \n",
    "# beam search\n",
    "def beam_search_decoder(data, k):\n",
    " sequences = [[list(), 0.0]]\n",
    " # walk over each step in sequence\n",
    " for row in data:\n",
    " all_candidates = list()\n",
    " # expand each current candidate\n",
    " for i in range(len(sequences)):\n",
    " seq, score = sequences[i]\n",
    " for j in range(len(row)):\n",
    " candidate = [seq + [j], score - log(row[j])]\n",
    " all_candidates.append(candidate)\n",
    " # order all candidates by score\n",
    " ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    " # select k best\n",
    " sequences = ordered[:k]\n",
    " return sequences\n",
    "\n",
    "\n",
    "# define a sequence of 10 words over a vocab of 5 words\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    " [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "data = array(data)\n",
    "# decode sequence\n",
    "result = beam_search_decoder(data, 3)\n",
    "# print result\n",
    "for seq in result:\n",
    " print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n",
    "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n",
    "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
