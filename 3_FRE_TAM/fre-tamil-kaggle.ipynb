{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9498165,"sourceType":"datasetVersion","datasetId":5780071}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport json\nimport re\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:03:28.993279Z","iopub.execute_input":"2024-09-28T00:03:28.993986Z","iopub.status.idle":"2024-09-28T00:03:41.232957Z","shell.execute_reply.started":"2024-09-28T00:03:28.993943Z","shell.execute_reply":"2024-09-28T00:03:41.232074Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"path =\"/kaggle/input/french-tamil-google-translations/french_tamil_0.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:03:43.295546Z","iopub.execute_input":"2024-09-28T00:03:43.296150Z","iopub.status.idle":"2024-09-28T00:03:43.300576Z","shell.execute_reply.started":"2024-09-28T00:03:43.296113Z","shell.execute_reply":"2024-09-28T00:03:43.299634Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path,index_col = 'Unnamed: 0')\n\nprint('length of df: ',len(df))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:03:44.405378Z","iopub.execute_input":"2024-09-28T00:03:44.406261Z","iopub.status.idle":"2024-09-28T00:03:44.992418Z","shell.execute_reply.started":"2024-09-28T00:03:44.406218Z","shell.execute_reply":"2024-09-28T00:03:44.991501Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"length of df:  125548\n","output_type":"stream"}]},{"cell_type":"code","source":"my_list1 = list(df['French'])\nmy_list1 = [x for y in my_list1 for x in y.split(' ')]\nmy_list1 = list(set(my_list1))\n\nmy_list2 = list(df['Tamil'])\nmy_list2 = [x for y in my_list2 for x in y.split(' ')]\nmy_list2 = list(set(my_list2))\n\nprint('approx length of french vocabulary: ',len(my_list1))\nprint('approx size of five_letter_words in fre_vocabulary',len([x for x in my_list1 if len(x)==5]))\nprint('approx length of Tamil vocabulary: ',len(my_list1))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:03:46.253545Z","iopub.execute_input":"2024-09-28T00:03:46.254569Z","iopub.status.idle":"2024-09-28T00:03:46.635837Z","shell.execute_reply.started":"2024-09-28T00:03:46.254509Z","shell.execute_reply":"2024-09-28T00:03:46.634848Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"approx length of french vocabulary:  31609\napprox size of five_letter_words in fre_vocabulary 2977\napprox length of Tamil vocabulary:  31609\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:03:48.625180Z","iopub.execute_input":"2024-09-28T00:03:48.626035Z","iopub.status.idle":"2024-09-28T00:03:48.639117Z","shell.execute_reply.started":"2024-09-28T00:03:48.625993Z","shell.execute_reply":"2024-09-28T00:03:48.638212Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                 French  \\\n0    Tout le monde n’est pas comme toi.   \n1  L'avion volait au-dessus des nuages.   \n2                    Reste loin de moi.   \n3                      Où as-tu eu ça ?   \n4              Je n'avais pas le choix.   \n\n                                             Tamil  \n0                   எல்லோரும் உங்களைப் போல் இல்லை.  \n1  விமானம் மேகங்களுக்கு மேல் பறந்து கொண்டிருந்தது.  \n2                          என்னை விட்டு விலகி இரு.  \n3                       இது எங்கிருந்து கிடைத்தது?  \n4                           எனக்கு வேறு வழியில்லை.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>French</th>\n      <th>Tamil</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Tout le monde n’est pas comme toi.</td>\n      <td>எல்லோரும் உங்களைப் போல் இல்லை.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>L'avion volait au-dessus des nuages.</td>\n      <td>விமானம் மேகங்களுக்கு மேல் பறந்து கொண்டிருந்தது.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Reste loin de moi.</td>\n      <td>என்னை விட்டு விலகி இரு.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Où as-tu eu ça ?</td>\n      <td>இது எங்கிருந்து கிடைத்தது?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Je n'avais pas le choix.</td>\n      <td>எனக்கு வேறு வழியில்லை.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"my_list = list(df['Tamil'].apply(lambda x: re.findall('[a-zA-Z]+',x)))\nprint('rows that contain english alphabets in tamil: ',len([x for x in my_list if len(x)]))\nindexes = []\nfor i,j in enumerate(my_list):\n    if len(j):\n        # print(j)\n        indexes.append(i)\ndf.drop(df.index[indexes],inplace =True,axis = 0)\n\n#cross checking once again\nmy_list = list(df['Tamil'].apply(lambda x: re.findall('[a-zA-Z]+',x)))\nprint('rows that contain english alphabets in tamil after elimination: ',len([x for x in my_list if len(x)]))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:03:49.835234Z","iopub.execute_input":"2024-09-28T00:03:49.835747Z","iopub.status.idle":"2024-09-28T00:03:51.086623Z","shell.execute_reply.started":"2024-09-28T00:03:49.835706Z","shell.execute_reply":"2024-09-28T00:03:51.085632Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"rows that contain english alphabets in tamil:  388\nrows that contain english alphabets in tamil after elimination:  0\n","output_type":"stream"}]},{"cell_type":"code","source":"french_list = df['French'].tolist()\ntamil_list = df['Tamil'].tolist()\nprint(french_list[:5])\nprint(tamil_list[:5])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:03:51.395046Z","iopub.execute_input":"2024-09-28T00:03:51.395404Z","iopub.status.idle":"2024-09-28T00:03:51.406820Z","shell.execute_reply.started":"2024-09-28T00:03:51.395369Z","shell.execute_reply":"2024-09-28T00:03:51.405878Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['Tout le monde n’est pas comme toi.', \"L'avion volait au-dessus des nuages.\", 'Reste loin de moi.', 'Où as-tu eu ça ?', \"Je n'avais pas le choix.\"]\n['எல்லோரும் உங்களைப் போல் இல்லை.', 'விமானம் மேகங்களுக்கு மேல் பறந்து கொண்டிருந்தது.', 'என்னை விட்டு விலகி இரு.', 'இது எங்கிருந்து கிடைத்தது?', 'எனக்கு வேறு வழியில்லை.']\n","output_type":"stream"}]},{"cell_type":"code","source":"text_pairs =list(zip(french_list,tamil_list))\nrandom.seed(42)\nrandom.shuffle(text_pairs)\ntext_pairs = text_pairs[:100000]\nprint(text_pairs[:5])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:04:21.845370Z","iopub.execute_input":"2024-09-28T00:04:21.845784Z","iopub.status.idle":"2024-09-28T00:04:21.987115Z","shell.execute_reply.started":"2024-09-28T00:04:21.845748Z","shell.execute_reply":"2024-09-28T00:04:21.986036Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[('Appelez la police!', 'காவல்துறையை அழைக்கவும்!'), ('Être conscient de ce que nous mangeons et de quelle quantité est essentiel à une bonne santé.', 'நாம் என்ன சாப்பிடுகிறோம், எவ்வளவு சாப்பிடுகிறோம் என்பதை அறிந்திருப்பது நல்ல ஆரோக்கியத்திற்கு அவசியம்.'), ('Elle a enseigné la musique pendant trente ans.', 'முப்பது வருடங்கள் இசை கற்பித்தார்.'), ('Tom a perdu contact avec Mary.', 'டாம் மேரி உடனான தொடர்பை இழந்தார்.'), ('Tom veut que son père soit enterré à côté de sa mère.', 'டாம் தனது தந்தையை தனது தாயின் அருகில் அடக்கம் செய்ய விரும்புகிறார்.')]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nnum_val_samples = int(0.1 * len(text_pairs))\nnum_train_samples = len(text_pairs) -  2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples+num_val_samples:]\n\nprint(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f'{len(test_pairs)} test pairs')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:04:25.215379Z","iopub.execute_input":"2024-09-28T00:04:25.215763Z","iopub.status.idle":"2024-09-28T00:04:25.224274Z","shell.execute_reply.started":"2024-09-28T00:04:25.215728Z","shell.execute_reply":"2024-09-28T00:04:25.223309Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"100000 total pairs\n80000 training pairs\n10000 validation pairs\n10000 test pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size_1 = 10000\nvocab_size_2 = 12000\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:04:27.333657Z","iopub.execute_input":"2024-09-28T00:04:27.334083Z","iopub.status.idle":"2024-09-28T00:04:27.340096Z","shell.execute_reply.started":"2024-09-28T00:04:27.334040Z","shell.execute_reply":"2024-09-28T00:04:27.337985Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def tf_lower_and_split_punct(text):\n    text = tf.strings.lower(text)\n    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n    text = tf.strings.strip(text)\n    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:04:34.956530Z","iopub.execute_input":"2024-09-28T00:04:34.956911Z","iopub.status.idle":"2024-09-28T00:04:34.962566Z","shell.execute_reply.started":"2024-09-28T00:04:34.956875Z","shell.execute_reply":"2024-09-28T00:04:34.961545Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def tf_lower_and_split_punct_1(text):\n    text = tf.strings.lower(text)\n    text = tf.strings.strip(text)\n    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:04:37.215588Z","iopub.execute_input":"2024-09-28T00:04:37.216565Z","iopub.status.idle":"2024-09-28T00:04:37.221784Z","shell.execute_reply.started":"2024-09-28T00:04:37.216523Z","shell.execute_reply":"2024-09-28T00:04:37.220820Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# vectorization\nfre_vectorization = tf.keras.layers.TextVectorization(\n    max_tokens = vocab_size_1,\n    output_mode = \"int\",\n    ragged = True,\n    standardize=tf_lower_and_split_punct\n)\n\ntam_vectorization = tf.keras.layers.TextVectorization(\n    max_tokens = vocab_size_2,\n    output_mode = \"int\",\n    ragged = True,\n    standardize=tf_lower_and_split_punct_1\n)\n\ntrain_fre_texts = [pair[0] for pair in train_pairs]\ntrain_tam_texts = [pair[1] for pair in train_pairs]\n\nfre_vectorization.adapt(train_fre_texts)\ntam_vectorization.adapt(train_tam_texts)\n\n#","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:04:46.225373Z","iopub.execute_input":"2024-09-28T00:04:46.225766Z","iopub.status.idle":"2024-09-28T00:04:47.843880Z","shell.execute_reply.started":"2024-09-28T00:04:46.225733Z","shell.execute_reply":"2024-09-28T00:04:47.843053Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# #save the vectorization layers\n# fre_vectorization_config = fre_vectorization.get_config()\n# fre_vectorization_config.pop('standardize', None)\nfre_vocab = fre_vectorization.get_vocabulary()\n# with open('fre_vectorization_config.json', 'w', encoding='utf-8') as f:\n#     json.dump(fre_vectorization_config, f)\n    \n# with open('fre_vocab.json', 'w', encoding='utf-8') as f:\n#     json.dump(fre_vocab, f)\n    \n# tam_vectorization_config = tam_vectorization.get_config()\n# tam_vectorization_config.pop('standardize', None)\ntam_vocab = tam_vectorization.get_vocabulary()\n# with open('tam_vectorization_config.json', 'w', encoding='utf-8') as f:\n#     json.dump(tam_vectorization_config, f)\n    \n# with open('tam_vocab.json', 'w', encoding='utf-8') as f:\n#     json.dump(tam_vocab, f)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:09.686510Z","iopub.execute_input":"2024-09-28T00:05:09.686887Z","iopub.status.idle":"2024-09-28T00:05:09.784167Z","shell.execute_reply.started":"2024-09-28T00:05:09.686851Z","shell.execute_reply":"2024-09-28T00:05:09.783379Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(len(fre_vocab))\nprint(len(tam_vocab))\nprint(fre_vocab[:20])\nprint(tam_vocab[:20])\nfive_letter_words = [x for x in fre_vocab if len(x)==5]\nprint('size of five letter words in french_vocab: ',len(five_letter_words))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:11.815634Z","iopub.execute_input":"2024-09-28T00:05:11.816386Z","iopub.status.idle":"2024-09-28T00:05:11.824523Z","shell.execute_reply.started":"2024-09-28T00:05:11.816348Z","shell.execute_reply":"2024-09-28T00:05:11.823543Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"10000\n12000\n['', '[UNK]', '[SOS]', '[EOS]', '.', 'de', 'je', 'tom', 'pas', 'la', '?', 'le', 'a', 'que', 'ne', 'il', 'est', 'vous', 'un', ',']\n['', '[UNK]', '[SOS]', '[EOS]', '.', 'நான்', 'டாம்', '?', 'நீங்கள்', 'ஒரு', 'என்று', 'வேண்டும்', ',', 'அவர்', 'எனக்கு', 'இந்த', 'உங்கள்', 'என்', 'மிகவும்', 'அவள்']\nsize of five letter words in french_vocab:  1655\n","output_type":"stream"}]},{"cell_type":"code","source":"Buffer_size = 10000\nbatch_size = 64\ndef generate_dataset(pairs):\n    fre_data = [x[0] for x in pairs]\n    tam_data = [x[1] for x in pairs]\n    dataset = tf.data.Dataset.from_tensor_slices((fre_data,tam_data)).shuffle(Buffer_size).batch(batch_size)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:26.755641Z","iopub.execute_input":"2024-09-28T00:05:26.756013Z","iopub.status.idle":"2024-09-28T00:05:26.762220Z","shell.execute_reply.started":"2024-09-28T00:05:26.755978Z","shell.execute_reply":"2024-09-28T00:05:26.761169Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_dataset = generate_dataset(train_pairs)\nval_dataset = generate_dataset(val_pairs)\ntest_dataset = generate_dataset(test_pairs)\nprint(len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:33.205914Z","iopub.execute_input":"2024-09-28T00:05:33.206843Z","iopub.status.idle":"2024-09-28T00:05:33.950398Z","shell.execute_reply.started":"2024-09-28T00:05:33.206801Z","shell.execute_reply":"2024-09-28T00:05:33.949467Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"157\n","output_type":"stream"}]},{"cell_type":"code","source":"def process(fre_data,tam_data):\n    fre_ids = fre_vectorization(fre_data).to_tensor()\n    tam_ids = tam_vectorization(tam_data)\n    target_in = tam_ids[:,:-1].to_tensor()\n    target_out = tam_ids[:,1:].to_tensor()\n    return (fre_ids,target_in),target_out","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:35.705547Z","iopub.execute_input":"2024-09-28T00:05:35.705908Z","iopub.status.idle":"2024-09-28T00:05:35.711690Z","shell.execute_reply.started":"2024-09-28T00:05:35.705875Z","shell.execute_reply":"2024-09-28T00:05:35.710636Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(process,tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(process,tf.data.AUTOTUNE)\ntest_dataset = test_dataset.map(process,tf.data.AUTOTUNE)\n\nprint(len(train_dataset))\nprint(len(val_dataset))\nprint(len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:37.604195Z","iopub.execute_input":"2024-09-28T00:05:37.604606Z","iopub.status.idle":"2024-09-28T00:05:37.959041Z","shell.execute_reply.started":"2024-09-28T00:05:37.604568Z","shell.execute_reply":"2024-09-28T00:05:37.958101Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"1250\n157\n157\n","output_type":"stream"}]},{"cell_type":"code","source":"for (x,y),z in train_dataset.take(1):\n    print(x[:2])\n    print(y[:2])\n    print(z[:2])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:40.515875Z","iopub.execute_input":"2024-09-28T00:05:40.516749Z","iopub.status.idle":"2024-09-28T00:05:40.624724Z","shell.execute_reply.started":"2024-09-28T00:05:40.516706Z","shell.execute_reply":"2024-09-28T00:05:40.623797Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[[   2   99  175    3    0    0    0    0    0    0    0    0    0    0\n     0    0]\n [   2    9 4684   33   11 6382 1281 1305  184  844    4    3    0    0\n     0    0]], shape=(2, 16), dtype=int64)\ntf.Tensor(\n[[   2 1043  364    0    0    0    0    0    0    0    0]\n [   2    1  934    1    1    1    4    0    0    0    0]], shape=(2, 11), dtype=int64)\ntf.Tensor(\n[[1043  364    3    0    0    0    0    0    0    0    0]\n [   1  934    1    1    1    4    3    0    0    0    0]], shape=(2, 11), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"word_to_id = tf.keras.layers.StringLookup(\n    vocabulary = tam_vocab,\n    mask_token = \"\",\n    oov_token = '[UNK]'\n)\n\nid_to_word = tf.keras.layers.StringLookup(\n    vocabulary = tam_vocab,\n    mask_token = '',\n    oov_token = '[UNK]',\n    invert = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:44.246655Z","iopub.execute_input":"2024-09-28T00:05:44.247512Z","iopub.status.idle":"2024-09-28T00:05:44.394175Z","shell.execute_reply.started":"2024-09-28T00:05:44.247458Z","shell.execute_reply":"2024-09-28T00:05:44.393256Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def decode_string(ints):\n  strs = [chr(i) for i in ints]\n  joined = ''.join(strs)\n  return joined\n\ndef tokens_to_text(tokens, id_to_word):\n    words = id_to_word(tokens)\n\n    try:\n       result = tf.strings.reduce_join(words, axis=-1, separator=\" \").numpy()\n    except:\n      result = words.numpy()\n\n    decoded = tf.strings.unicode_decode(result,'utf-8').numpy()\n    decoded_sentence = decode_string(decoded)\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:45.325614Z","iopub.execute_input":"2024-09-28T00:05:45.325988Z","iopub.status.idle":"2024-09-28T00:05:45.333186Z","shell.execute_reply.started":"2024-09-28T00:05:45.325953Z","shell.execute_reply":"2024-09-28T00:05:45.332207Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sos_id = word_to_id('[SOS]')\neos_id = word_to_id('[EOS]')\nprint(sos_id)\nprint(eos_id)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:46.395478Z","iopub.execute_input":"2024-09-28T00:05:46.396383Z","iopub.status.idle":"2024-09-28T00:05:47.516236Z","shell.execute_reply.started":"2024-09-28T00:05:46.396341Z","shell.execute_reply":"2024-09-28T00:05:47.515313Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"tf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size_1 = 10000\nvocab_size_2 = 12000\nunits_1 = 128","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:47.518045Z","iopub.execute_input":"2024-09-28T00:05:47.518695Z","iopub.status.idle":"2024-09-28T00:05:47.522755Z","shell.execute_reply.started":"2024-09-28T00:05:47.518648Z","shell.execute_reply":"2024-09-28T00:05:47.521874Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self,fre_vocab_size = vocab_size_1,units = units_1):\n        super(Encoder,self).__init__()\n        \n        self.vocab_size = fre_vocab_size\n        self.units =units\n\n        self.embedding = tf.keras.layers.Embedding(input_dim = fre_vocab_size,output_dim = units,mask_zero=True)\n        self.lstm = tf.keras.layers.Bidirectional(merge_mode='sum',layer = tf.keras.layers.LSTM(units,return_sequences= True))\n\n    def call(self,encoder_inputs):\n\n        embedded_output = self.embedding(encoder_inputs)\n        output = self.lstm(embedded_output)\n        return output\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:49.735598Z","iopub.execute_input":"2024-09-28T00:05:49.736333Z","iopub.status.idle":"2024-09-28T00:05:49.743996Z","shell.execute_reply.started":"2024-09-28T00:05:49.736293Z","shell.execute_reply":"2024-09-28T00:05:49.743011Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_size_1,units_1)\noutput1 = encoder(x)\noutput1.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:50.603508Z","iopub.execute_input":"2024-09-28T00:05:50.603860Z","iopub.status.idle":"2024-09-28T00:05:51.660384Z","shell.execute_reply.started":"2024-09-28T00:05:50.603828Z","shell.execute_reply":"2024-09-28T00:05:51.659410Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TensorShape([64, 16, 128])"},"metadata":{}}]},{"cell_type":"code","source":"class CrossAttention(tf.keras.layers.Layer):\n    def __init__(self,units=units_1):\n        super().__init__()\n\n        self.units =units\n\n        self.mha = (tf.keras.layers.MultiHeadAttention(key_dim= units,num_heads=1))\n        self.layernorm = tf.keras.layers.LayerNormalization()\n        self.add = tf.keras.layers.Add()\n        self.support_masking = True\n    def call(self,context,target):\n\n        attn_output = self.mha(query = target,value = context)\n        x = self.add([target,attn_output])\n        x = self.layernorm(x)\n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:51.661891Z","iopub.execute_input":"2024-09-28T00:05:51.662183Z","iopub.status.idle":"2024-09-28T00:05:51.669447Z","shell.execute_reply.started":"2024-09-28T00:05:51.662153Z","shell.execute_reply":"2024-09-28T00:05:51.668428Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"attention = CrossAttention(units_1)\nembed_output = tf.keras.layers.Embedding(vocab_size_2,units_1)(y)\noutput2  = attention(output1,embed_output)\noutput2.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:52.205610Z","iopub.execute_input":"2024-09-28T00:05:52.206235Z","iopub.status.idle":"2024-09-28T00:05:52.871032Z","shell.execute_reply.started":"2024-09-28T00:05:52.206196Z","shell.execute_reply":"2024-09-28T00:05:52.870113Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'cross_attention' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TensorShape([64, 11, 128])"},"metadata":{}}]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self,fre_vocab_size = vocab_size_1,units = units_1,tam_vocab_size = vocab_size_2):\n        super(Decoder,self).__init__()\n\n        self.fre_vocab_size = fre_vocab_size\n        self.tam_vocab_size = tam_vocab_size\n        self.units = units\n\n        self.embedding = tf.keras.layers.Embedding(input_dim = tam_vocab_size,output_dim = units,mask_zero=True)\n        self.pre_attention_rnn = tf.keras.layers.LSTM(units,return_sequences = True,return_state = True)\n        self.attention = CrossAttention(units)\n        self.post_attention_rnn = tf.keras.layers.LSTM(units = units,return_sequences=True)\n        self.dense = tf.keras.layers.Dense(tam_vocab_size,activation = tf.nn.log_softmax)\n        self.support_masking = True\n    def call(self,context,target,state = None,return_state = False):\n\n        embedding_output = self.embedding(target)\n        x,state_h,state_c = self.pre_attention_rnn(embedding_output,initial_state=state)\n        x = self.attention(context,x)\n        x = self.post_attention_rnn(x)\n        logits = self.dense(x)\n\n        if return_state:\n            return logits,[state_h,state_c]\n\n        return logits\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:53.095697Z","iopub.execute_input":"2024-09-28T00:05:53.096344Z","iopub.status.idle":"2024-09-28T00:05:53.105700Z","shell.execute_reply.started":"2024-09-28T00:05:53.096301Z","shell.execute_reply":"2024-09-28T00:05:53.104638Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"decoder = Decoder(vocab_size_1,units_1)\noutput3 = decoder(output1,y)\noutput3.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:53.715634Z","iopub.execute_input":"2024-09-28T00:05:53.716440Z","iopub.status.idle":"2024-09-28T00:05:54.127444Z","shell.execute_reply.started":"2024-09-28T00:05:53.716395Z","shell.execute_reply":"2024-09-28T00:05:54.126525Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'cross_attention_1' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'decoder' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TensorShape([64, 11, 12000])"},"metadata":{}}]},{"cell_type":"code","source":"class Translator(tf.keras.Model):\n    def __init__(self,fre_vocab_size =vocab_size_1,units = units_1,tam_vocab_size = vocab_size_2):\n        super().__init__()\n        self.encoder = Encoder(fre_vocab_size,units)\n        self.decoder = Decoder(fre_vocab_size,units,tam_vocab_size)\n\n    def call(self,inputs):\n        context,target = inputs\n        encoder_output = self.encoder(context)\n        logits = self.decoder(encoder_output,target)\n\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:56.813847Z","iopub.execute_input":"2024-09-28T00:05:56.814233Z","iopub.status.idle":"2024-09-28T00:05:56.820897Z","shell.execute_reply.started":"2024-09-28T00:05:56.814195Z","shell.execute_reply":"2024-09-28T00:05:56.819892Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"translator = Translator(vocab_size_1,units_1,vocab_size_2)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:05:58.966757Z","iopub.execute_input":"2024-09-28T00:05:58.967148Z","iopub.status.idle":"2024-09-28T00:05:59.003941Z","shell.execute_reply.started":"2024-09-28T00:05:58.967109Z","shell.execute_reply":"2024-09-28T00:05:59.003226Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def compile_and_train(model,epochs =10,steps_per_epoch = 500):\n    model.compile(optimizer = 'adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction = 'none'),\n                  metrics = ['accuracy'])\n\n    history  = model.fit(\n        train_dataset.repeat(),\n        epochs = epochs,\n        steps_per_epoch = steps_per_epoch,\n        validation_data = val_dataset,\n        validation_steps = 50,\n        callbacks = [tf.keras.callbacks.EarlyStopping(patience=3)]\n    )\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:06:04.031116Z","iopub.execute_input":"2024-09-28T00:06:04.031935Z","iopub.status.idle":"2024-09-28T00:06:04.038020Z","shell.execute_reply.started":"2024-09-28T00:06:04.031895Z","shell.execute_reply":"2024-09-28T00:06:04.036977Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#repeat this step 3-4 times i.e for 30-40 epochs\ntrained_translator,history = compile_and_train(translator,epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:19:30.360959Z","iopub.execute_input":"2024-09-28T00:19:30.361680Z","iopub.status.idle":"2024-09-28T00:20:56.865787Z","shell.execute_reply.started":"2024-09-28T00:19:30.361638Z","shell.execute_reply":"2024-09-28T00:20:56.864929Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - accuracy: 0.9303 - loss: 0.3032 - val_accuracy: 0.8540 - val_loss: 0.8719\nEpoch 2/20\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 34ms/step - accuracy: 0.9325 - loss: 0.3020 - val_accuracy: 0.8510 - val_loss: 0.8869\nEpoch 3/20\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 34ms/step - accuracy: 0.9353 - loss: 0.2896 - val_accuracy: 0.8466 - val_loss: 0.9329\nEpoch 4/20\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - accuracy: 0.9417 - loss: 0.2575 - val_accuracy: 0.8503 - val_loss: 0.8922\n","output_type":"stream"}]},{"cell_type":"code","source":"trained_translator.save_weights('french_to_tamil.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:21:26.222015Z","iopub.execute_input":"2024-09-28T00:21:26.223049Z","iopub.status.idle":"2024-09-28T00:21:26.381459Z","shell.execute_reply.started":"2024-09-28T00:21:26.222994Z","shell.execute_reply":"2024-09-28T00:21:26.380632Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def generate_next_token_0(decoder,context,next_token,done,state):\n    \n    logits,state = decoder(context,next_token,state,return_state = True)\n\n    logits = logits[:,-1,:]\n\n    next_token = tf.argmax(logits,axis = -1)\n\n    logits = tf.squeeze(logits)\n\n    next_token = tf.squeeze(next_token)\n\n    logit = logits[next_token].numpy()\n    \n    next_token = tf.reshape(next_token,shape  = (1,1))\n    \n    if next_token == eos_id :\n        done = True\n    return next_token,state,done,logit","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:21:28.737205Z","iopub.execute_input":"2024-09-28T00:21:28.737922Z","iopub.status.idle":"2024-09-28T00:21:28.744333Z","shell.execute_reply.started":"2024-09-28T00:21:28.737879Z","shell.execute_reply":"2024-09-28T00:21:28.743305Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def translate_0(model,text,max_length = 30,):\n    tokens,logits = [],[]\n    #condition to convert only five letter words\n    text = text.split(' ')\n    text = [x.strip().strip(',').strip('?').strip('!').strip('\"').strip('.') for x in text]\n    text = [x for x in text if len(x)==5]\n    # print('five _letter_words in input_text: ' ,len(text))\n    if len(text) == 0:\n        return 'The Text has no five letter words! Please try again'\n    text = ' '.join(text)\n    text = tf.convert_to_tensor(text)[tf.newaxis]\n    context = fre_vectorization(text).to_tensor()\n    context = model.encoder(context)\n    state = [tf.zeros((1,units_1)),tf.zeros((1,units_1))]\n    next_token = tf.fill((1,1),sos_id)\n    done  = False\n\n    for i in range(max_length):\n        try:\n            next_token,state,done,logit = generate_next_token_0(decoder = model.decoder,\n                                                                context = context,\n                                                                next_token = next_token,\n                                                                done = done,\n                                                                state = state,\n                                                              )\n        except:\n            raise Exception('generate next token code issue')\n        if done:\n            break\n        tokens.append(next_token)\n        logits.append(logit)\n    tokens = tf.concat(tokens,axis = -1)\n    tokens = tf.squeeze(tokens)\n    \n    translation =tokens_to_text(tokens,id_to_word)\n\n    return translation,","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:27:55.982285Z","iopub.execute_input":"2024-09-28T00:27:55.982690Z","iopub.status.idle":"2024-09-28T00:27:55.994615Z","shell.execute_reply.started":"2024-09-28T00:27:55.982652Z","shell.execute_reply":"2024-09-28T00:27:55.993768Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"my_list = [\"S'il vous plaît, allez-y.\",\n           \"êtres\",\n           \"Je pense que je deviens fou.\",\n           \"Vous devriez arrêter de fumer car c'est malsain.\",\n           \"autre bûche\",\n           \" Je jouais de la flûte quand j'étais au lycée\",\n           ' cible quels huilé',\n           \"belle fille noirs était\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:27:56.892215Z","iopub.execute_input":"2024-09-28T00:27:56.893024Z","iopub.status.idle":"2024-09-28T00:27:56.897852Z","shell.execute_reply.started":"2024-09-28T00:27:56.892975Z","shell.execute_reply":"2024-09-28T00:27:56.896828Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"for i in my_list[:]:\n    translation = translate_0(trained_translator,i,)\n    print(translation)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:27:58.776846Z","iopub.execute_input":"2024-09-28T00:27:58.777219Z","iopub.status.idle":"2024-09-28T00:27:59.875525Z","shell.execute_reply.started":"2024-09-28T00:27:58.777184Z","shell.execute_reply":"2024-09-28T00:27:59.874728Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"('தயவுசெய்து',)\n('உயிரினங்கள்',)\n('நினைக்கிறார்கள்',)\n('இதை புகை',)\n('மற்றொரு பதிவு',)\n('உயர்நிலைப் பள்ளி [UNK] வந்த உடைந்த இடம்',)\n('எண்ணெய் தடவிய இலக்கு யாரென்று தொடர்',)\n('அழகான கருப்பு பெண்',)\n","output_type":"stream"}]},{"cell_type":"code","source":"fre_data = [x[0] for x in train_pairs]\ntam_data = [x[1] for x in train_pairs]\nprint(len(fre_data))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:28:04.897068Z","iopub.execute_input":"2024-09-28T00:28:04.897948Z","iopub.status.idle":"2024-09-28T00:28:04.929330Z","shell.execute_reply.started":"2024-09-28T00:28:04.897905Z","shell.execute_reply":"2024-09-28T00:28:04.928355Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"80000\n","output_type":"stream"}]},{"cell_type":"code","source":"p = 67890\nprint(fre_data[p])\nprint(tam_data[p])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:28:06.916939Z","iopub.execute_input":"2024-09-28T00:28:06.917861Z","iopub.status.idle":"2024-09-28T00:28:06.922727Z","shell.execute_reply.started":"2024-09-28T00:28:06.917816Z","shell.execute_reply":"2024-09-28T00:28:06.921668Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Surtout, soyez patient.\nஎல்லாவற்றிற்கும் மேலாக, பொறுமையாக இருங்கள்.\n","output_type":"stream"}]},{"cell_type":"code","source":"translate_0(trained_translator,fre_data[p])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:27:40.601907Z","iopub.execute_input":"2024-09-28T00:27:40.602268Z","iopub.status.idle":"2024-09-28T00:27:40.728211Z","shell.execute_reply.started":"2024-09-28T00:27:40.602234Z","shell.execute_reply":"2024-09-28T00:27:40.727262Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"('[UNK] சிறந்தது',)"},"metadata":{}}]},{"cell_type":"code","source":"fre_data[p]","metadata":{"execution":{"iopub.status.busy":"2024-09-28T00:23:50.102792Z","iopub.execute_input":"2024-09-28T00:23:50.103190Z","iopub.status.idle":"2024-09-28T00:23:50.109682Z","shell.execute_reply.started":"2024-09-28T00:23:50.103152Z","shell.execute_reply":"2024-09-28T00:23:50.108662Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"\"Tom est assez souvent en retard à l'école.\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"output_shape = 10 * 26 \noutput = [[0.1, 0.01, 0.03, ... ... ... ... ... (len-26)],\n          [0.1, 0.002, 0.6, ... ... ... ... ... (len-26)],\n            ''         ''         ''      ''       ''    \n            ''         ''         ''      ''       '' \n          [0.9, 0.01, 0.01, ... ... ... ... ... (len-26)]]","metadata":{}},{"cell_type":"markdown","source":"m","metadata":{}},{"cell_type":"markdown","source":"import numpy as np\ndef greedy_search_decoder(predictions):\n  \n    #select token with the maximum probability for each prediction\n    output_sequence = [np.argmax(prediction) for prediction in predictions]\n    \n    #storing token probabilities\n    token_probabilities = [np.max(prediction) for prediction in predictions]\n    \n    #multiply individaul token-level probabilities to get overall sequence probability\n    sequence_probability = np.product(token_probabilities)\n    \n    return output_sequence, sequence_probability\n    \nmodel_prediction = [[0.1, 0.7, 0.1, 0.1],\n                    [0.7, 0.1, 0.1, 0.1],\n                    [0.1, 0.1, 0.6, 0.2],\n                    [0.1, 0.1, 0.1, 0.7],\n                    [0.4, 0.3, 0.2, 0.1]]\n                    \ngreedy_search_decoder(model_prediction)\n\n","metadata":{}},{"cell_type":"markdown","source":"import numpy as np\nimport math\n\ndef beam_search_decoder(predictions, top_k = 3):\n    #start with an empty sequence with zero score\n    output_sequences = [([], 0)]\n    \n    #looping through all the predictions\n    for token_probs in predictions:\n        new_sequences = []\n        \n        #append new tokens to old sequences and re-score\n        for old_seq, old_score in output_sequences:\n            for char_index in range(len(token_probs)):\n                new_seq = old_seq + [char_index]\n                #considering log-likelihood for scoring\n                new_score = old_score + math.log(token_probs[char_index])\n                new_sequences.append((new_seq, new_score))\n                \n        #sort all new sequences in the de-creasing order of their score\n        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n        \n        #select top-k based on score \n        # *Note- best sequence is with the highest score\n        output_sequences = output_sequences[:top_k]\n        \n    return output_sequences\n    \n\nmodel_prediction = [[0.1, 0.7, 0.1, 0.1],\n                    [0.7, 0.1, 0.1, 0.1],\n                    [0.1, 0.1, 0.6, 0.2],\n                    [0.1, 0.1, 0.1, 0.7],\n                    [0.4, 0.3, 0.2, 0.1]]\n                    \nbeam_search_decoder(model_prediction, top_k = 5)\n\n","metadata":{}},{"cell_type":"markdown","source":"[Out] : [([1, 0, 2, 3, 0], -2.497141187456343),\n         ([1, 0, 2, 3, 1], -2.784823259908124),\n         ([1, 0, 2, 3, 2], -3.1902883680162883),\n         ([1, 0, 3, 3, 0], -3.595753476124453),\n         ([1, 0, 2, 3, 3], -3.8834355485762337)]\n\n\n[Out]: ([1, 0, 2, 3, 0], 0.08231999999999998)","metadata":{}},{"cell_type":"markdown","source":"data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1]]\ndata = array(data)","metadata":{}},{"cell_type":"markdown","source":"def greedy_decoder(data):\n # index for largest probability each row\n return [argmax(s) for s in data]","metadata":{}},{"cell_type":"markdown","source":"from numpy import argmax\n \n# greedy decoder\ndef greedy_decoder(data):\n # index for largest probability each row\n return [argmax(s) for s in data]\n \n# define a sequence of 10 words over a vocab of 5 words\ndata = [[0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1]]\ndata = array(data)\n# decode sequence\nresult = greedy_decoder(data)","metadata":{}},{"cell_type":"markdown","source":"[4, 0, 4, 0, 4, 0, 4, 0, 4, 0]","metadata":{}},{"cell_type":"markdown","source":"# beam search\ndef beam_search_decoder(data, k):\n sequences = [[list(), 0.0]]\n # walk over each step in sequence\n for row in data:\n all_candidates = list()\n # expand each current candidate\n for i in range(len(sequences)):\n seq, score = sequences[i]\n for j in range(len(row)):\n candidate = [seq + [j], score - log(row[j])]\n all_candidates.append(candidate)\n # order all candidates by score\n ordered = sorted(all_candidates, key=lambda tup:tup[1])\n # select k best\n sequences = ordered[:k]\n return sequences","metadata":{}},{"cell_type":"markdown","source":"from math import log\nfrom numpy import array\nfrom numpy import argmax\n \n# beam search\ndef beam_search_decoder(data, k):\n sequences = [[list(), 0.0]]\n # walk over each step in sequence\n for row in data:\n all_candidates = list()\n # expand each current candidate\n for i in range(len(sequences)):\n seq, score = sequences[i]\n for j in range(len(row)):\n candidate = [seq + [j], score - log(row[j])]\n all_candidates.append(candidate)\n # order all candidates by score\n ordered = sorted(all_candidates, key=lambda tup:tup[1])\n # select k best\n sequences = ordered[:k]\n return sequences\n\n\n# define a sequence of 10 words over a vocab of 5 words\ndata = [[0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1],\n [0.1, 0.2, 0.3, 0.4, 0.5],\n [0.5, 0.4, 0.3, 0.2, 0.1]]\ndata = array(data)\n# decode sequence\nresult = beam_search_decoder(data, 3)\n# print result\nfor seq in result:\n print(seq)","metadata":{}},{"cell_type":"markdown","source":"\n[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}